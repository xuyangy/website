---
title: "WGS Upscaling - IT & Bioinformatics Evaluation"
title-block-banner: true
description: "Data transfer, Data storage, Bioinformatics pipeline capacity"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx, Oslo University Hospital"
      url: "https://dpipe.gitlab.io/docs/docs-pages"
      role: "collect data"
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
#     fig-cap-location: top
date: last-modified
format: 
    html:
        theme: Journal
        toc: true
        number-sections: true
        number-depth: 3
        float: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
---

# Background
GDx at OUSAMG is planning to upscale the WGS production to `4 x 48` samples or `2 x 48 + 1 x 96` samples per week. 

This document evaluates the possible bottlenecks of IT & bioinformatics pipelines in following areas:

1. Data transfer speed
2. Data storage
3. Pipeline capacity (Illumina DRAGEN)

# IT && Bioinformatics



## Data transfer speed

```{r}
#| echo: false
#| message: false
#| warning: false
recognize_bandwidth <- function(bandwidth_string) {
  # Use regular expression to extract numeric part and unit part
  match_result <- regexec("^([0-9.]+)\\s*([KMGT]?B)(/s)?$", bandwidth_string)

  if (match_result[[1]][1] == -1) {
    stop("Invalid bandwidth string format")
  }

  # Extract numeric and unit parts
  numeric_part <- as.numeric(regmatches(bandwidth_string, match_result)[[1]][2])
  unit_part <- regmatches(bandwidth_string, match_result)[[1]][3]

  # Convert to bytes per second
  bytes_per_second <- switch(unit_part,
    "B" = numeric_part,
    "KB" = numeric_part * 1000,
    "MB" = numeric_part * 1000^2,
    "GB" = numeric_part * 1000^3,
    stop("Invalid unit")                                     
  )                                                          
                                                             
  return(bytes_per_second)                                   
}                                                            
                                                             
transform_bandwidth <- function(bandwidth) {
  return(lapply(bandwidth, recognize_bandwidth))
}

# load data from csv file
library(readr)               
library(dplyr)               
data <- read_csv("small.csv", col_names = TRUE, col_types = "cccddc")
data <- mutate(data, speed = sapply(speed, recognize_bandwidth))
```


:::{.panel-tabset}

## [Data Collection]{style="font-family:Chalkboard; font-size:20px;"}


To evaluate the data transfer speed, we collected the transfer time for each file transferred from NSC to TSD between [`r strptime(min(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} and [`r strptime(max(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"}.


## [Examples]{style="font-family:Chalkboard; font-size:20px;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(stringr)
t(data |> filter(str_ends(data$filename, ".vcf")) |> slice_sample(n = 1))
t(data |> filter(str_ends(data$filename, ".fastq.gz")) |> slice_sample(n = 1))
t(data |> filter(str_ends(data$filename, ".bam")) |> slice_sample(n = 1))
```
:::
                             
::: {.callout-note collapse="true"}
### This is an info callout.
some other
:::

### Transfer of large files over 200 MB
In total $4443$ files larger than 200 MB were transferred with an average speed of $78 MB/s$.


wgs326	2023-11-01 14:13:42,747	 2023-11-02 20:14:43,258
wgs327	2023-11-03 12:11:16,334	
