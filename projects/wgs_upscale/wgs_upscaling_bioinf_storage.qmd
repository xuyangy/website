---
title: "WGS Upscaling - NSC Storage Capacity"
title-block-banner: true
image: "storage.png"
description: "NSC storage capability"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx OUSAMG"
      url: "https://www.robotgenome.com"
      role: "collect data"
date: last-modified
format: 
    html:
        css: style.css
        toc: true
        toc-depth: 6
        toc-expand: true
        number-sections: true
        number-depth: 6
        float: true
        smooth-scroll: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
            <link href="https://fonts.googleapis.com/css2?family=Krona+One&display=swap" rel="stylesheet">
comments: 
    hypothesis: false
execute: 
    cache: false
    echo: false
    message: false
    warning: false
---

# Background
<hr>

GDx at OUSAMG is planning to upscale the WGS production to `192` samples (`4 x 48` or `2 x 48 + 1 x 96`) samples per week. 
Do we have enough capacity in IT and bioinformatics pipelines for this upscaling?

The capacity of IT & bioinformatics pipelines can be evaluated from following three aspects:

1. [Data transfer speed](https://robotgenome.com/projects/wgs_upscale/wgs_upscaling_bioinf.html)
2. Data storage
3. [Pipeline capacity](https://robotgenome.com/projects/wgs_upscale/wgs_upscaling_bioinf_pipeline.html)


**This document will focus on the evaluation of NSC storage capacity.**

# NSC Storage Capacity
<hr>

## Capacity breakdown
::: {.callout-warning}
### [As of Dec 29, 2024, 12:00 PM]{style="color:red"}
Total capacity of NSC storage is `552.6 TiB`. The capability breakdown as of Dec 29, 2024 12:00 PM is as follows:
:::

![](capacity_breakdown.png)


## Usable capacity
::: {.callout-warning}
### [As of Dec 29, 2024, 12:00 PM]{style="color:red"}
Usable capacity as of Dec 29, 2024, 12:00 PM is `231.2 TiB`.
:::

![](usable_capacity.png)


## Storage volumes

| Name                                                                                  | Purpose                               |
| -----                                                                                 | ---                                   |
| [/boston]{style="font-family:courier"}                                                | General data area                     |
| [/boston/runScrach]{style="font-family:courier"}                                      | Sequencing runs (Illumina, ONT)       |
| [/boston/projects]{style="font-family:courier"}                                       | Research projects                     |
| [/boston/common]{style="font-family:courier"}                                         | Software, and repositories            |
| [/boston/diag]{style="font-family:courier"}                                           | Diagnostics production                |
| [/boston/diag/transfer]{style="font-family:courier"}                                  | Transfer area (for TSD)               |
| [/boston/runScratch/demultiplexed/delivery/tsd_sleipnir]{style="font-family:courier"} | Transfer area NSC                     |
| [vm-datastore]{style="font-family:courier"}                                           | VMware datastore (virtual hard disks) |


## Used storage breakdown
::: {.callout-warning}
### [As of Dec 29, 2024, 12:00 PM:]{style="color:red"}
The used storage as of Dec 29, 2024, 12:00 PM is `228 TiB`. Details are as follows:
:::


```{python}
#| label: fig-nsc_storage_used
#| fig-cap: Used storage

import pandas as pd
import plotly.express as px

df = pd.read_csv(
    "nsc_storage.csv",
    dtype={"Folder": str, "Parent": str, "Size": float, "DisplaySize": str},
)
df = df.fillna("")
fig = px.sunburst(
    df, names="Folder", parents="Parent", values="Size", branchvalues="remainder"
)
disp = list(df["DisplaySize"])
fig.update_traces(
    hovertemplate="Folder: %{currentPath}%{label}<br>Size: %{text} (%{percentParent:.1%})",
    text=disp,
)
fig.update_layout(margin=dict(t=20, l=0, r=0, b=0), height=900)
```

::: {style="font-family:courier"}
    
### [/boston]{style="font-family:courier"} (228 TiB)    
| Directory                                             | Logical  | %use of Parent Directory | Physical |
| -----------                                           | -----    | --------                 | -----    |
| [/boston/]{.commonDir}[diag](#boston-diag)            | 96.9 TiB | 51.8%                    | 113 TiB  |
| [/boston/]{.commonDir}[runScrach](#boston-runscratch) | 83.1 TiB | 44.4%                    | 107 TiB  |
| [/boston/]{.commonDir}projects                        | 2.87 TiB | 1.5%                     | 3.74 TiB |
| [/boston/]{.commonDir}home                            | 2.77 TiB | 1.5%                     | 2.82 TiB |
| [/boston/]{.commonDir}common                          | 1.42 TiB | 0.8%                     | 1.54 TiB |
: {.striped .hover}

:::

#### [/boston/diag]{#boston-diag style="font-family:courier"}

::: {style="font-family:courier"}
| Directory                                                        | Logical  | %use of Parent Directory | Physical |
| -----------                                                      | -----    | --------                 | -----    |
| [/boston/diag/]{.commonDir}runs                                  | 35.8 TiB | 37.0%                    | 47.4 TiB |
| [/boston/diag/]{.commonDir}[production](#boston-diag-production) | 33.8 TiB | 34.9%                    | 34.3 TiB |
| [/boston/diag/]{.commonDir}[transfer](#boston-diag-transfer)     | 15.6 TiB | 16.1%                    | 16.3 TiB |
| [/boston/diag/]{.commonDir}nscDelivery                                         | 7.82 TiB | 8.1%                     | 10.4 TiB |
| [/boston/diag/]{.commonDir}[staging](#boston-diag-staging)       | 3.84 TiB | 4.0%                     | 4.88 TiB |
| [/boston/diag/]{.commonDir}diagInternal                          | 1.48 GiB | 0.0%                     | 1.88 GiB |
: {.striped .hover}
:::


##### [/boston/diag/production]{#boston-diag-production style="font-family:courier"}


::: {style="font-family:courier"}
| Directory                                         | Logical  | %use of Parent Directory | Physical |
| -----------                                       | -----    | --------                 | -----    |
| [/boston/diag/production/]{.commonDir}data        | 33.5 TiB | 99.1%                    | 33.9 TiB |
| [/boston/diag/production/]{.commonDir}sw          | 238  GiB | 0.7%                     | 297  GiB |
| [/boston/diag/production/]{.commonDir}reference   | 74.3 GiB | 0.2%                     | 63.9 GiB |
| [/boston/diag/production/]{.commonDir}logs        | 3.32 GiB | 0.0%                     | 557  MiB |
| [/boston/diag/production/]{.commonDir}.thirdparty | 110  MiB | 0.0%                     | 148  GiB |
:::


##### [/boston/diag/transfer]{#boston-diag-transfer style="font-family:courier"}
::: {style="font-family:courier"}
| Directory                                      | Logical  | %use of Parent Directory | Physical |
| -----------                                    | -----    | --------                 | -----    |
| [/boston/diag/transfer/]{.commonDir}production | 14.8 TiB | 97.7%                    | 15.4 TiB |
: {.striped .hover}
:::


##### [/boston/diag/staging]{#boston-diag-staging style="font-family:courier"}
::: {style="font-family:courier"}
| Directory                                    | Logical  | %use of Parent Directory | Physical |
| -----------                                  | -----    | --------                 | -----    |
| [/boston/diag/]{.commonDir}staging/data      | 3.53 TiB | 92.0%                    | 4.54 TiB |
| [/boston/diag/]{.commonDir}staging/sw        | 237 GiB  | 6.0%                     | 285 GiB  |
| [/boston/diag/]{.commonDir}staging/reference | 77.5 GiB | 2.0%                     | 67.5 GiB |
: {.striped .hover}
:::


#### [/boston/runScratch]{#boston-runscratch style="font-family:courier"}
::: {style="font-family:courier"}
| Directory                                         | Logical  | %use of Parent Directory | Physical |
| -----------                                       | -----    | --------                 | -----    |
| [/boston/runScratch/]{.commonDir}NovaSeqX         | 34.5 TiB | 41.5%                    | 45.7 TiB |
| [/boston/runScratch/]{.commonDir}analysis         | 38.8 TiB | 37.0%                    | 37.1 TiB |
| [/boston/runScratch/]{.commonDir}demultiplexed    | 15.6 TiB | 18.8%                    | 20.7 TiB |
| [/boston/runScratch/]{.commonDir}processed        | 1.03 TiB | 1.2%                     | 1.58 TiB |
| [/boston/runScratch/]{.commonDir}ONT              | 738 GiB  | 0.9%                     | 1.02 TiB |
| [/boston/runScratch/]{.commonDir}UserData         | 244 GiB  | 0.3%                     | 254 GiB  |
| [/boston/runScratch/]{.commonDir}test             | 64.7 GiB | 0.1%                     | 86.1 GiB |
| [/boston/runScratch/]{.commonDir}PGT              | 16.7 GiB | 0.0%                     | 20.5 GiB |
| [/boston/runScratch/]{.commonDir}Upgrade_software | 16.7 GiB | 0.0%                     | 22.2 GiB |
| [/boston/runScratch/]{.commonDir}mik_data         | 12.5 GiB | 0.0%                     | 12.5 GiB |
| [/boston/runScratch/]{.commonDir}imm_data         | 4.5 GiB  | 0.0%                     | 4.53 GiB |
: {.striped .hover}
:::


## Expected data
The data generated by NovaSeqX depend on the settings of secondary analysis and the sequencing depth.
Current setting is 64 samples per flowcell.

### Per sample

#### BCL Convert
The `{R1,R2}.fastq.ora` files per sample is about `14 GB` data (`18 GB` on disk).

Other files such as BCL files, images, logs, reports, etc. is about `47 GB` data per sample (`63 GB` on disk).

::: {.callout-important}
### Data
[In total, `61 GB` data per sample (`81 GB` on disk).]{#bcl-convert-data-1}
:::

With BCL Convert only, we need to run inhouse pipelines on external DRAGEN.

Given the design of the inhouse pipelines, some files are duplicated in different locations.

1. Only 1 copy of any _fastq.ora_ file is physically stored on boston, _i.e.,_ not duplicated. A _fastq.ora_ file appears in 4 different locations:

    (@) `/boston/diag/nscDelivery`
    (@) `/boston/diag/transfer/production/{normal,high,urgent}/samples` or `/boston/daig/transfer/production/transferred/{normal,high,urgent}/samples`
    (@) `/boston/diag/production/data/samples`
    (@) `/boston/diag/production/data/analyses-work/\*/result/\*/work`

    [1]{.mono}, [2]{.mono} and [3]{.mono} are hardlinks; [4]{.mono} is symlink of [3]{.mono} (within `/boston/diag/produciton/data/analyses-work`, files are symlinked from the Nextflow [work]{.mono} folder.)

2. Files in `/boston/diag/produciton/data/analyses-results/{singles,trios}` are copies of that in `/boston/diag/produciton/data/analyses-work`

3. Files in `/boston/diag/transfer/production/{normal,high,urgent}/analyses-results/{singles,trios}` are copies of that in `/boston/diag/produciton/data/analyses-work`


##### analyses-work folder size
```{python}
#| cache: false
import plotly.graph_objects as go
from numpy import mean

analyses_work_dirs_basepipe_data = [63, 63, 66, 66, 66, 66, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78, 78, 80, 80, 82]
mean_analyses_work_dirs_basepipe_data = mean(analyses_work_dirs_basepipe_data)
analyses_work_dirs_basepipe_disk= [61, 61, 64, 64, 65, 65, 65, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 75, 75, 76, 76, 77, 77, 78, 78, 78, 79]
analyses_work_dirs_triopipe_data = [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 13]
analyses_work_dirs_triopipe_disk=[7.1, 7.1, 7.1, 7.1, 7.1, 7.1, 7.1, 7.2, 7.2, 7.2, 7.3, 7.3, 8.3]
analyses_work_dirs_annopipe_data = [6.7, 6.7, 6.7, 6.7, 6.7, 6.7, 6.7, 6.7, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.8, 6.9, 6.9, 6.9, 6.9, 6.9, 6.9, 6.9, 6.9, 7.0, 7.0, 7.0, 7.0, 7.0, 7.2, 7.3, 7.4, 7.4, 7.4, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 21]
analyses_work_dirs_annopipe_disk=[6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.1, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.2, 6.3, 6.3, 6.3, 6.4, 6.4, 6.4, 6.5, 6.6, 6.7, 6.7, 6.7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19]

analyses_results_singles_data = [48, 48, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61, 61, 61, 62, 62, 63]
analyses_results_singles_disk= [48, 48, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61, 61, 61, 62, 62, 63]
analyses_results_trios_data = [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.7, 2.7, 2.7, 2.7, 2.7, 3.1]
analyses_results_trios_disk= [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.7, 2.7, 2.7, 2.7, 2.7, 3.1]

ella_incoming_disk = [3.9, 4.1, 4.3, 4.8, 4.9, 5.6, 6.1, 6.7, 7.0, 7.4, 7.7, 8.0, 8.0, 8.2, 8.3, 9.7, 9.9, 9.9, 11, 11, 11, 11, 11, 12, 13, 14, 15, 16, 18, 25, 25, 26, 29, 31, 31, 34, 34, 35, 36, 43, 67, 67, 68, 128, 129, 130, 466, 470, 471, 471, 471, 475, 475, 477, 479, 482, 482, 489, 566]
ella_incoming_data =  [3.3, 3.4, 3.7, 4.2, 4.4, 5.2, 5.5, 6.3, 6.6, 7.0, 7.2, 7.6, 7.6, 7.8, 8.0, 9.3, 9.5, 9.7, 9.9, 10, 11, 11, 11, 12, 12, 14, 15, 15, 18, 25, 25, 26, 28, 30, 31, 34, 34, 35, 36, 43, 66, 66, 67, 128, 128, 129, 464, 469, 470, 470, 470, 473, 474, 475, 478, 480, 481, 487, 565]


fig1 = go.Figure(
    data=[
        go.Box(y=analyses_work_dirs_basepipe_data, boxpoints="all", name="basepipe(data)"),
        go.Box(y=analyses_work_dirs_basepipe_disk, boxpoints="all", name="basepipe(on disk)"),
        go.Box(y=analyses_work_dirs_triopipe_data, boxpoints="all", name="triopipe(data)"),
        go.Box(y=analyses_work_dirs_triopipe_disk, boxpoints="all", name="triopipe(on disk)"),
        go.Box(y=analyses_work_dirs_annopipe_data, boxpoints="all", name="annopipe(data)"),
        go.Box(y=analyses_work_dirs_annopipe_disk, boxpoints="all", name="annopipe(on disk)"),
    ]
)
fig1 = fig1.update_layout(
    title="analyses-work directories size (GB)",
    yaxis_title="Size (GB)",
    height=900,
    width=1050,
    title_x=0.5,
    margin=dict(t=50, l=0, r=0, b=50),
    template="ggplot2",
)
# fig1 = fig1.update_traces(hovertemplate="Size: %{y} GB")
fig1.show()

```

::: {.callout-important}
### Data
```{python}
from IPython.display import display, Markdown

display(
    Markdown(
        "Average analyses-work basepipe folder size is `%d GB` (`%d GB` on disk).\n\n"
        "Average analyses-work triopipe folder size is `%d GB` (`%d GB` on disk).\n\n"
        "Average analyses-work annopipe folder size is `%d GB` (`%d GB` on disk).\n\n"
        "Adding [NovaSeqX generated data](#bcl-convert-data-1), the total data per sample is `%d GB` (`%d GB` on disk)."
        % (
            mean(analyses_work_dirs_basepipe_data),
            mean(analyses_work_dirs_basepipe_disk),
            mean(analyses_work_dirs_triopipe_data),
            mean(analyses_work_dirs_triopipe_disk),
            mean(analyses_work_dirs_annopipe_data),
            mean(analyses_work_dirs_annopipe_disk),
            61
            + mean(analyses_work_dirs_basepipe_data)
            + sum(analyses_work_dirs_triopipe_data)
            / len(analyses_work_dirs_basepipe_data)
            + sum(analyses_work_dirs_annopipe_data)
            / len(analyses_work_dirs_basepipe_data),
            81
            + mean(analyses_work_dirs_basepipe_disk)
            + sum(analyses_work_dirs_triopipe_disk)
            / len(analyses_work_dirs_basepipe_disk)
            + sum(analyses_work_dirs_annopipe_disk)
            / len(analyses_work_dirs_basepipe_disk),
        )
    )
)
```
 
:::

##### analyses-results folder size
```{python}
#| cache: false
import plotly.graph_objects as go

analyses_results_singles_data = [48, 48, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61, 61, 61, 62, 62, 63]
analyses_results_singles_disk= [48, 48, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61, 61, 61, 62, 62, 63]
analyses_results_trios_data = [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.7, 2.7, 2.7, 2.7, 2.7, 3.1]
analyses_results_trios_disk= [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.7, 2.7, 2.7, 2.7, 2.7, 3.1]

ella_incoming_disk = [3.9, 4.1, 4.3, 4.8, 4.9, 5.6, 6.1, 6.7, 7.0, 7.4, 7.7, 8.0, 8.0, 8.2, 8.3, 9.7, 9.9, 9.9, 11, 11, 11, 11, 11, 12, 13, 14, 15, 16, 18, 25, 25, 26, 29, 31, 31, 34, 34, 35, 36, 43, 67, 67, 68, 128, 129, 130, 466, 470, 471, 471, 471, 475, 475, 477, 479, 482, 482, 489, 566]
ella_incoming_data =  [3.3, 3.4, 3.7, 4.2, 4.4, 5.2, 5.5, 6.3, 6.6, 7.0, 7.2, 7.6, 7.6, 7.8, 8.0, 9.3, 9.5, 9.7, 9.9, 10, 11, 11, 11, 12, 12, 14, 15, 15, 18, 25, 25, 26, 28, 30, 31, 34, 34, 35, 36, 43, 66, 66, 67, 128, 128, 129, 464, 469, 470, 470, 470, 473, 474, 475, 478, 480, 481, 487, 565]


fig1 = go.Figure(
    data=[
        go.Box(y=analyses_results_singles_data, boxpoints="all", name="singles(data)"),
        go.Box(y=analyses_results_singles_disk, boxpoints="all", name="singles(on disk)"),
        go.Box(y=analyses_results_trios_data, boxpoints="all", name="trios(data)"),
        go.Box(y=analyses_results_trios_disk, boxpoints="all", name="trios(on disk)"),
    ]
)
fig1.update_layout(
    title="analyses-results directories size (GB)",
    yaxis_title="Size (GB)",
    height=600,
    width=1050,
    title_x=0.5,
    template="ggplot2",
)
fig1.update_traces(hovertemplate="Size: %{y} GB")
fig1.show()
```


::: {.callout-important}
### Data
```{python}
from IPython.display import display, Markdown

display(
    Markdown(
        "Average analyses-results singles folder size is `%d GB` (`%d GB` on disk).\n\n"
        "Average analyses-results trios folder size is `%d GB` (`%d GB` on disk).\n\n"
        % (
            mean(analyses_results_singles_data),
            mean(analyses_results_singles_disk),
            mean(analyses_results_trios_data),
            mean(analyses_results_trios_disk),
        )
    )
)
```
:::


##### ella-incoming folder size
```{python}
#| cache: false
import plotly.graph_objects as go

ella_incoming_data =  [3.3, 3.4, 3.7, 4.2, 4.4, 5.2, 5.5, 6.3, 6.6, 7.0, 7.2, 7.6, 7.6, 7.8, 8.0, 9.3, 9.5, 9.7, 9.9, 10, 11, 11, 11, 12, 12, 14, 15, 15, 18, 25, 25, 26, 28, 30, 31, 34, 34, 35, 36, 43, 66, 66, 67, 128, 128, 129, 464, 469, 470, 470, 470, 473, 474, 475, 478, 480, 481, 487, 565]
ella_incoming_disk = [3.9, 4.1, 4.3, 4.8, 4.9, 5.6, 6.1, 6.7, 7.0, 7.4, 7.7, 8.0, 8.0, 8.2, 8.3, 9.7, 9.9, 9.9, 11, 11, 11, 11, 11, 12, 13, 14, 15, 16, 18, 25, 25, 26, 29, 31, 31, 34, 34, 35, 36, 43, 67, 67, 68, 128, 129, 130, 466, 470, 471, 471, 471, 475, 475, 477, 479, 482, 482, 489, 566]


fig1 = go.Figure(
    data=[
        go.Box(y=ella_incoming_data, boxpoints="all", name="data"),
        go.Box(y=ella_incoming_disk, boxpoints="all", name="on disk"),
    ]
)
fig1.update_layout(
    title="ella-incoming directories size (MB)",
    yaxis_title="Size (MB)",
    height=600,
    width=1050,
    title_x=0.5,
    template="ggplot2",
)
fig1.update_traces(hovertemplate="Size: %{y} MB")
fig1.show()
```

::: {.callout-important}
### Data
```{python}
from IPython.display import display, Markdown

display(
    Markdown(
        "Average ella-incoming folder size is `%d MB` (`%d MB` on disk).\n\n"
        % (
            mean(ella_incoming_data),
            mean(ella_incoming_disk),
        )
    )
)
```
:::


#### DRAGEN Germline
When secondary analysis is DRAGEN Germline with all variant callers, _i.e._ doing both demultiplexing and mapping, variant
calling with Onboard DRAGEN.

Each sample has about `20 GB` pipeline output data (`27 GB` on disk) in addition.
See @sec-analysis-folder for details.

::: {.callout-important}
### Data
In total, `81 GB` data per sample (`108 GB` on disk).
:::

##### Pipeline output files per sample{#sec-analysis-folder}

::: {style="font-family: courier;font-size:0.7rem"}
>analysis/wgs435_HB12345678_b08a0667-b221-48c6-8d44-abb516a61a2b/germline_seq    
>├── [  20G]  germline_seq    
│   ├── [ 2.6M]  report.html    
│   ├── [  13M]  sv    
│   │   ├── [ 6.1M]  results    
│   │   │   ├── [  44K]  stats    
│   │   │   │   ├── [  736]  alignmentStatsSummary.txt    
│   │   │   │   ├── [  20K]  candidate_metrics.csv    
│   │   │   │   ├── [  535]  diploidSV.sv_metrics.csv    
│   │   │   │   ├── [ 4.4K]  graph_metrics.csv    
│   │   │   │   ├── [ 9.0K]  svCandidateGenerationStats.tsv    
│   │   │   │   ├── [ 6.7K]  svCandidateGenerationStats.xml    
│   │   │   │   └── [ 1.7K]  svLocusGraphStats.tsv    
│   │   │   └── [ 6.1M]  variants    
│   │   │       ├── [ 4.2M]  candidateSV.vcf.gz    
│   │   │       ├── [ 671K]  candidateSV.vcf.gz.tbi    
│   │   │       ├── [ 1.1M]  diploidSV.vcf.gz    
│   │   │       └── [ 121K]  diploidSV.vcf.gz.tbi    
│   │   └── [ 6.5M]  workspace    
│   │       ├── [  56K]  alignmentStats.xml    
│   │       ├── [  505]  chromDepth.txt    
│   │       ├── [  59K]  edgeRuntimeLog.txt    
│   │       ├── [  17K]  genomeSegmentScanDebugInfo.txt    
│   │       ├── [ 2.3K]  logs    
│   │       │   └── [ 2.2K]  config_log.txt    
│   │       └── [ 6.4M]  svLocusGraph.bin    
│   ├── [ 182K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.excluded_intervals.bed.gz    
│   ├── [ 457K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.gff3    
│   ├── [ 2.8K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.igv_session.xml    
│   ├── [  793]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv_metrics.csv    
│   ├── [  70K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.vcf.gz    
│   ├── [   32]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.vcf.gz.md5sum    
│   ├── [  18K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cnv.vcf.gz.tbi    
│   ├── [  [16G]{style="color:red"}]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cram    
│   ├── [ 1.3M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cram.crai    
│   ├── [   32]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cram.md5sum    
│   ├── [  302]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cyp2b6.tsv    
│   ├── [  283]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.cyp2d6.tsv    
│   ├── [ 420K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.fastqc_metrics.csv    
│   ├── [ 272K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.fragment_length_hist.csv    
│   ├── [  185]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.gba.tsv    
│   ├── [ 2.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.gvcf_hethom_ratio_metrics.csv    
│   ├── [ 2.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.gvcf_metrics.csv    
│   ├── [  46M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.baf.bw    
│   ├── [ [3.8G]{style="color:red"}]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.gvcf.gz    
│   ├── [   32]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.gvcf.gz.md5sum    
│   ├── [ 1.2M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.gvcf.gz.tbi    
│   ├── [ 365M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.vcf.gz    
│   ├── [   32]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.vcf.gz.md5sum    
│   ├── [ 1.6M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.hard-filtered.vcf.gz.tbi    
│   ├── [  16M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.improper.pairs.bw    
│   ├── [  429]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.insert-stats.tab    
│   ├── [ 8.9K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.mapping_metrics.csv    
│   ├── [ 9.2K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.metrics.json    
│   ├── [  39K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.pcr-model-0.log    
│   ├── [  115]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.pcr-model.log    
│   ├── [ 1.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.ploidy_estimation_metrics.csv    
│   ├── [ 1.8K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.ploidy.vcf.gz    
│   ├── [   32]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.ploidy.vcf.gz.md5sum    
│   ├── [ 4.0K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.ploidy.vcf.gz.tbi    
│   ├── [ 1.9M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.repeats.bam    
│   ├── [ 4.3K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.repeats.vcf.gz    
│   ├── [ 3.9K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.repeats.vcf.gz.tbi    
│   ├── [  48K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.roh.bed    
│   ├── [  114]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.roh_metrics.csv    
│   ├── [ 242K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.seg    
│   ├── [  69K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.seg.bw    
│   ├── [ 247K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.seg.called    
│   ├── [ 259K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.seg.called.merged    
│   ├── [  223]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.smn.tsv    
│   ├── [ 1.7K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.snperror-sampler.log    
│   ├── [  535]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.sv_metrics.csv    
│   ├── [ 1.1M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.sv.vcf.gz    
│   ├── [ 121K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.sv.vcf.gz.tbi    
│   ├── [  19M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.target.counts.bw    
│   ├── [  22M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.target.counts.diploid.bw    
│   ├── [  31M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.target.counts.gc-corrected.gz    
│   ├── [  25M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.target.counts.gz    
│   ├── [ 1.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.targeted.json    
│   ├── [  22M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.tn.bw    
│   ├── [  37M]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.tn.tsv.gz    
│   ├── [ 1.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.trimmer_metrics.csv    
│   ├── [ 7.2K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.vc_hethom_ratio_metrics.csv    
│   ├── [ 2.4K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.vc_metrics.csv    
│   ├── [ 2.8K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.wgs_contig_mean_cov.csv    
│   ├── [ 2.1K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.wgs_coverage_metrics.csv    
│   ├── [  16K]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.wgs_fine_hist.csv    
│   ├── [  558]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.wgs_hist.csv    
│   └── [   43]  wgs435_HB12345678_3c52f4e7-826d-40dd-bd98-f4356deeb098.wgs_overall_mean_cov.csv    
└── [  11K]  logs  
\ \ \ \ ├── [  258]  bcl2fastq.dragen_events.csv  
\ \ \ \ ├── [ 1.1K]  cmdline_1198902.txt  
\ \ \ \ ├── [  608]  cmdline_1608030.txt    
\ \ \ \ ├── [ 1.1K]  cmdline_3358454.txt    
\ \ \ \ ├── [  219]  DCKR_RG-stderr_1608030.txt    
\ \ \ \ ├── [  461]  DCKR_RG-stdout_1608030.txt    
\ \ \ \ ├── [  250]  ora.dragen_events.csv    
\ \ \ \ ├── [    1]  ORA-stderr_1198902.txt    
\ \ \ \ ├── [ 3.9K]  ORA-stdout_1198902.txt    
\ \ \ \ ├── [    1]  P2FSW-stderr_3358454.txt    
\ \ \ \ └── [ 2.4K]  P2FSW-stdout_3358454.txt    
:::

::: {.callout-tip}
👆 Generated by `tree --du` which shows the actual file sizes instead of disk space used.
:::


### Per sequencing run
Each sequencing run can have different set up.

- Side: 
  - single side (single flowcell) 
  - both sides (dual flowcell)
- Flowcell type: 
  - 1.5B
  - 10B
  - 25B
- Secondary analysis:
  - BCL Convert
  - DRAGEN Germline
    - None
    - SmallVariantCaller
    - AllVariantCallers (Small, Structural, CNV, Repeat Expansions, ROH, CYP2D6.)

#### Single 25B flowcell; BCL Convert
Single 25B flowcell (64 samples), secondary analysis is BCL Convert, _i.e._ only demultiplexing.

::: {.callout-important}
### Data
`3.9 TB` data (`5.1 TB` on disk).
:::

#### Dual 25B flowcell; BCL Convert
Dual 25B flowcell (128 samples), secondary analysis is BCL Convert, _i.e._ only demultiplexing.

::: {.callout-important}
### Data
`7.8 TB` data (`10.2 TB` on disk).
:::


#### Dual 25B flowcell; DRAGEN Germline, AllVariantCallers
Dual 25B flowcell (128 samples), secondary analysis is DRAGEN Germline with all variant callers enabled, _i.e._ doing both
demultiplexing and mapping, variant calling with Onboard DRAGEN.

::: {.callout-important}
### Data
`10.5 TB` data (`14.0 TB` on disk).
:::

