---
title: "WGS Upscaling - IT & Bioinformatics Evaluation"
title-block-banner: true
image: "wgsupscale.png"
css: style.css
description: "Data transfer, Data storage, Bioinformatics pipeline capacity"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx OUSAMG"
      url: "https://www.robotgenome.com"
      role: "collect data"
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
#     fig-cap-location: top
date: last-modified
format: 
    html:
        toc: true
        toc-depth: 4
        toc-expand: true
        number-sections: true
        number-depth: 4
        float: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
            <link href="https://fonts.googleapis.com/css2?family=Krona+One&display=swap" rel="stylesheet">
comments: 
    hypothesis: true
execute: 
    cache: true
    echo: false
    message: false
    warning: false
---

# Background
<hr>

GDx at OUSAMG is planning to upscale the WGS production to `192` samples (`4 x 48` or `2 x 48 + 1 x 96`) samples per week. 
Do we have enough capacity in IT and bioinformatics pipelines for this upscaling?

The capacity of IT & bioinformatics pipelines can be evaluated from following three aspects:

1. **Data transfer speed**
2. **Data storage**
3. **Pipeline capacity**


This document will focus on the evaluation of data transfer speed.

# Data transfer speed
<hr>

Both _sequencing data_ and _NSC pipeline results_ are stored at the NSC and need to be transferred to the TSD.
The amount of data to be transferred is very large.
The data transfer is done by the **nsc-exporter** software which uses TSD s3api, a wrapper of s3cmd. 
The nsc-exporter will check for new data to transfer every 10 minutes and uses `s3cmd put` to transfer the data.

<br>

```{mermaid}
flowchart LR
    subgraph dt[**nsc-exporter**]
    check-new-data(((data to transfer))) --> |Yes| transfer(Transfer data to TSD) 
    transfer --> sleep(Sleep 10 minutes)
    sleep --> |check| check-new-data
    check-new-data --> |No| sleep
    end
    subgraph dp[**2 data producers**]
    lims-exporter[lims-exporter]  ---> |produce| check-new-data
    pipeline[NSC pipelines] ---> |produce| check-new-data
    end
    style dt fill:#e4eda6,stroke-width:3px
    style dp fill:#F2D1FF
```

<br>

```{r}
#| echo: false
#| message: false
#| warning: false
recognize_bandwidth <- function(bandwidth_string) {
  # Use regular expression to extract numeric part and unit part
  match_result <- regexec("^([0-9.]+)\\s*([KMGT]?B)(/s)?$", bandwidth_string)

  if (match_result[[1]][1] == -1) {
    stop("Invalid bandwidth string format")
  }

  # Extract numeric and unit parts
  numeric_part <- as.numeric(regmatches(bandwidth_string, match_result)[[1]][2])
  unit_part <- regmatches(bandwidth_string, match_result)[[1]][3]

  # Convert to bytes per second
  bytes_per_second <- switch(unit_part,
    "B" = numeric_part,
    "KB" = numeric_part * 1000,
    "MB" = numeric_part * 1000^2,
    "GB" = numeric_part * 1000^3,
    stop("Invalid unit")                                     
  )                                                          
                                                             
  return(bytes_per_second)                                   
}                                                            
                                                             
transform_bandwidth <- function(bandwidth) {
  return(lapply(bandwidth, recognize_bandwidth))
}

# load data from csv file
library(readr)               
library(dplyr)               
data <- read_csv("data.csv", col_names = TRUE, col_types = "cccddc")
data <- mutate(data, speed = sapply(speed, recognize_bandwidth))
```

## Data Collection

To evaluate the data transfer speed from NSC to TSD, we collected the historical data transfer records
  between [`r strptime(min(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} 
  and [`r strptime(max(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} 
  from the nsc-exporter log. 

::: {.callout-tip collapse="true"}

### expand to see examples of collected file transfer records


```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(stringr)
for (extension in c(".bam", ".fastq.gz", ".vcf", ".sample", ".pdf")) {
  print(t(data |> filter(str_ends(data$filename, extension)) |> slice_sample(n = 1)))
}
```

:::

::: {.callout-note}
### The nsc-exporter log files and the sequencer overview html files were ignored for simplicity. [^1]


:::
## Data Overview
::: {.panel-tabset}
### File Size
```{r}
#| cache: false
library(gdata)
```
The size of transferred files ranges from 
  [`r humanReadable(min(data$bytes))`]{style="font-family:Quicksand;"} to 
  [`r humanReadable(max(data$bytes))`]{style="font-family:Quicksand;"}. 
The average file size is [`r humanReadable(mean(data$bytes))`]{style="font-family:Quicksand;"}. 
The median file size is [`r humanReadable(median(data$bytes))`]{style="font-family:Quicksand;"}. 
The standard deviation is [`r humanReadable(sd(data$bytes))`]{style="font-family:Quicksand;"}.

:::: {.columns}
::: {.column width="30%"}
```{r} 
setNames(as.data.frame(sapply(summary(data$bytes), humanReadable)), "filesize")
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
library(ggplot2)
ggplot(
    data,
    aes(x = bytes)
) +
    geom_histogram(fill = "#fc03d3", bins = 200) +
    labs(x = "file size (bytes)", y = "file count", title = "File Size Distribution") + theme_minimal()
ggplot(
    data,
    aes(x = bytes)
) +
geom_histogram(fill = "#fc03d3", bins = 200) +
labs(
    x = "file size (bytes)",
    y = "file count (log10)",
    title = "File Size Distribution (log10 file count)"
) + 
scale_y_continuous(trans='log10') + theme_minimal()
```
:::
::::
### Transfer Speed
The transfer speed ranges 
  from [`r humanReadable(min(data$speed))`/s]{style="font-family:Quicksand;"}
  to [`r humanReadable(max(data$speed))`/s]{style="font-family:Quicksand;"}.
The average transfer speed is [`r humanReadable(mean(data$speed))`/s]{style="font-family:Quicksand;"}.
The median transfer speed is [`r humanReadable(median(data$speed))`/s]{style="font-family:Quicksand;"}.
The standard deviation is [`r humanReadable(sd(data$speed))`/s]{style="font-family:Quicksand;"}.
Transfer speed for small files is usually very low, so the average transfer speed is not a good indicator 
  for actual transfer speed, see @sec-correlation-filesize-speed-all.

:::: {.columns}
::: {.column width="30%"}
```{r} 
setNames(as.data.frame(sapply(summary(data$speed), humanReadable)), "speed(/s)")
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
ggplot(
  data,
  aes(x = speed)
) + 
  geom_histogram(bins = 200, fill = "#0bd440") + 
  labs(x = "bytes/s", y = "file count", title = "Transfer Speed") + 
  theme_minimal()

ggplot(
    data,
    aes(x = speed)
) + 
  geom_histogram(bins = 200, fill = "#0bd440") + 
  labs(
    x = "bytes/s",
    y = "file count (log10)",
    title = "Transfer Speed (log10 file count)"
  ) + 
  scale_y_continuous(trans = 'log10') +
  theme_minimal()
```
:::
::::
### Transfer Time
The transfer time ranges 
  from [`r round(min(data$seconds), 1)`]{style="font-family:Quicksand;"} seconds 
  to [`r round(max(data$seconds), 1)`]{style="font-family:Quicksand;"} seconds.
The average transfer time is [`r round(mean(data$seconds),1)`]{style="font-family:Quicksand;"} seconds.
The median transfer time is [`r round(median(data$seconds),1)`]{style="font-family:Quicksand;"} seconds.
The standard deviation is [`r round(sd(data$seconds),1)`]{style="font-family:Quicksand;"} seconds.

:::: {.columns}
::: {.column width="30%"}
```{r} 
summary(data %>% select(seconds))
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
ggplot(
  data,
  aes(x = seconds)
) + 
  geom_histogram(bins = 200, fill = "#e3571b") + 
  labs(x = "transfer time (per file)", y = "file count", title = "Transfer Time") + theme_minimal()

ggplot(
  data,
  aes(x = seconds)
) + 
  geom_histogram(bins = 200, fill = "#e3571b") + 
  labs(x = "transfer time (per file)", y = "file count (log10)", title = "Transfer Time (log10 file count)") + 
  scale_y_continuous(trans = "log10") + 
  theme_minimal()
```
:::
::::
:::

## Correlation

Next, we would like to know the how does file size affect transfer speed and transfer time.

<!-- read data from csv file -->
```{python}
#| cache: false
#| include: false
def deHumanReadable(bandwidth_string):
    k = 1024  # s3cmd uses 1024 
    match_result = re.match(r'^([0-9.]+)\s*([KMGT]?B)(/s)?$', bandwidth_string)

    if match_result is None:
        raise ValueError("Invalid bandwidth string format")

    numeric_part = float(match_result.group(1))
    unit_part = match_result.group(2)
    bytes_per_second = {
        "B": numeric_part,
        "KB": numeric_part * k,
        "MB": numeric_part * k**2,
        "GB": numeric_part * k**3
    }.get(unit_part, ValueError("Invalid unit"))

    return bytes_per_second

```

### Transfer speed and time VS file size (all files){#sec-correlation-filesize-speed-all}
<hr>

Small files have lower transfer speed. A good transfer speed around 80 MB/s can be achieved for large files (>2 GB).
However, the best speed is observed for files with size around 200 MB (zoom in or see @fig-correlation-filesize-speed-200MB).
```{python} 
#| label: fig-correlation-filesize-speed-all
#| fig-cap: Transfer speed VS file size (all files)
#| cache: false
import pandas as pd
from datetime import datetime
import re
import plotly.express as px
py_df = pd.read_csv("data.csv", 
                    sep=",", 
                    parse_dates=["datetime"],
                    date_format="%Y-%m-%d %H:%M:%S.%f")
py_df["speed_humanize"] = py_df["speed"]
py_df["speed"] = py_df["speed"].apply(deHumanReadable)
py_df["project_type"] = py_df["project"].apply(lambda x: re.match(r"[a-zA-Z]+", x).group())
fig = px.scatter(
    py_df,
    x="bytes",
    y="speed",
    labels={"bytes": "file size (bytes)", "speed": "transfer speed"},
    opacity=0.3,
    color="project_type",
    height=600,
)
fig.show()
```

```{r}
#| label: fig-correlation-filesize-time-all
#| fig-cap: Transfer time VS file size (all files)
#| cache: false
#| fig-width: 10
#| fig-height: 4
library(reticulate)
library(ggplot2)

ggplot(py$py_df, aes(x = bytes, y = seconds, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer time (seconds)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  guides(color = guide_legend(title = "Project Type")) + 
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
```
### Transfer speed and time VS file size (small files)
<hr>

Although the transfer speed of small files are very low; 
  the transfer time is usually very short. 
So small files are not the bottleneck of the data transfer. 
See also @sec-too-many-small-files.

```{r}
#| label: fig-correlation-filesize-speed-small
#| fig-cap: Transfer speed VS file size (small files)
#| fig-width: 10
#| fig-height: 4
#| cache: true
small_files <- py$py_df[py$py_df$bytes < 300000, ]
fig_sf <- ggplot(small_files, aes(x = bytes, y = speed, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer speed") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold")) +
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
print(fig_sf)
```

```{r}
#| label: fig-correlation-filesize-time-small
#| fig-cap: Transfer time VS file size (small files)
#| fig-width: 10
#| fig-height: 4

ggplot(small_files, aes(x = bytes, y = seconds, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer time (seconds)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
```


### [ Maximum transfer reached around <u>200MB</u> file size? ]{style="color:orange;"}
<hr>

Small files have lower transfer speed. 
Large files have higher transfer speed. 
But it looks like best transfer speed is observed for files with sizearound 200 MB file size.

```{r}
#| label: fig-correlation-filesize-speed-200MB
#| fig-cap: maximum transfer speed reached around 200MB file size
#| fig-width: 10
#| fig-height: 4

library(gdata)
small_boundary <- 500000000
small_files <- py$py_df[py$py_df$bytes < small_boundary, ]
x_breaks <- seq(0, small_boundary, 100000000)
y_breaks <- seq(0, max(small_files$speed), 10000000)

ggplot(small_files, aes(x = bytes, y = speed, color = project_type)) +
  geom_point(alpha = 0.3) + 
  scale_x_continuous(
    breaks = x_breaks, 
    labels = humanReadable(
      x_breaks,
      standard = "SI", 
      digits = 0)
  ) +
  scale_y_continuous(
    breaks = y_breaks, 
    labels = humanReadable(
      y_breaks,
      standard = "SI", 
      digits = 0)
  ) +
  labs(
    x = "file size (bytes)", 
    y = "transfer speed", 
    title = "Transfer speed maximizes at 200 MB file size"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  guides(color = guide_legend(title = "Project Type")) + 
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
```

## Idle Time {#sec-idle-time}

To evaluate whether there is capacity for upscaling, we need to know the idle time of the nsc-exporter. 
The nsc-exporter is idle when it is not transferring data.  

All transfer records are plotted with 
  starting time of each transfer on x-axis and 
  the time used to finished the transfer on y-axis. 
The gaps represents idle periods of nsc-exporter. 
The color represents projects, *e.g.* wgs123, EKG20230901 *etc.*. 
The shape represents project type, *e.g.* wgs, EKG *etc.* 
You can turn off a project by clicking it in the legend to the right of the figure.  

For easier visualization, the data is grouped in months.

```{python} 
#| cache: false
#| include: false
import humanize
from pathlib import Path
py_df['datetime_shifted']= py_df["datetime"].shift(1)
py_df['time_gap'] = (pd.to_datetime(py_df['datetime']) - pd.to_datetime(py_df['datetime_shifted']))[1:].reset_index(drop=True)/pd.Timedelta(1, "S")
py_df.datetime = py_df.datetime.shift().fillna(py_df.datetime[0])
py_df.time_gap = py_df.time_gap.shift().fillna(0)
py_df["file_extension"] = py_df.filename.apply(lambda x: Path(x).suffix if Path(x).suffix else x)
py_df["size"] = py_df.bytes.apply(humanize.naturalsize)
py_df[((py_df.time_gap - py_df.seconds) > 30) & ((py_df.time_gap - py_df.seconds) < 600)]

def md5(x):
  if (x.time_gap - x.seconds) > 5 and (x.time_gap - x.seconds) < 600:
    return x.time_gap - x.seconds
  else:
    return 0

py_df["md5_check"] = py_df.apply(md5, axis=1)
```
### September
<hr>

```{python} 
#| label: fig-timeseries-sep-trans-time
#| fig-cap: Transfer time of all files in September
py_df_sep = py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))]
px.scatter(
    py_df_sep,
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "time_gap", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

<div class="modal fade" id="exampleModal" tabindex="-1">
<div class="modal-dialog modal-dialog-centered p-8" style="max-width: 90%; max-height: 90%">
<div class="modal-content">

```{python}
#| out-height: 100%
#| out-width: 100%
py_df_sep = py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))]
fig = px.scatter(
    py_df_sep,
    x="datetime",
    y="seconds",
    opacity=0.5,
    color="project",
    symbol="project_type",
    hover_data=["size", "time_gap", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=1150,
)
fig.update_layout(
    autosize = True,
)
```

</div></div></div>

<button type="button" class="btn btn-success" data-bs-toggle="modal" data-bs-target="#exampleModal">
See in Full Screen
</button>


```{r} 
r_py_df_sep <- py$py_df_sep
```

```{python} 
#| label: fig-gap-sep
#| fig-cap: Idle time in September (transfer gaps)
px.scatter(
    py_df_sep,
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "seconds", "file_extension", "speed_humanize"],
    log_y=False,
    labels={"datetime": "transfer start datetime", "time_gap": "Interval to next transfer start (seconds)"},
    title="transfer gaps (time difference between two transfer starts)",
    height=600,
)
```

```{python} 
import datetime
#| cache: true
slept = pd.read_csv('sleeping_time.csv', parse_dates=['sleep_at'])
ttt_sep = sum(py_df_sep["seconds"])
ttmd5_sep = sum(py_df_sep["md5_check"])
ttdata_sep = humanize.naturalsize(sum(py_df_sep["bytes"]))
ttmd5_sep_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttmd5_sep))
ttt_sep_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttt_sep))
slept_times_sep = len(slept[(pd.to_datetime("2023-09-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-10-01")) & slept["small_sleep"]])
slept_tot_sep = slept_times_sep * 600
ttt_sep_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_sep))
ttp_sep = py_df_sep.groupby("project", as_index=False).count()
ttp_sep_wgs = len(ttp_sep[(ttp_sep["project"].apply(lambda x: x.startswith("wgs"))) & (ttp_sep["filename"]>2000)])
tteffective_sep = humanize.precisedelta(ttt_sep + slept_tot_sep + ttmd5_sep)
```

```{r} 
#| label: fig-md5-check-sep
#| fig-cap: Time used for md5sum check in September [^md5sum]
#| fig-width: 12
library(reticulate)
r_py_df_sep %>% 
  mutate(r_datetime=as.POSIXct(datetime, format="%Y-%m-%d %H:%M:%S")) %>% 
    filter(!(project_type == "wgs" & md5_check == 0)) %>% 
    ggplot(aes(x = r_datetime, y=md5_check, color=project_type)) + 
    geom_point() + 
    scale_x_datetime(date_minor_breaks = "1 day") + 
    labs(x = "datetime", y = "md5sum check time (seconds)", title = "Time used for md5sum check") +
    theme(plot.title = element_text(hjust = 0.5)) +
    guides(color = guide_legend(title = "Project Type")) + 
    scale_color_manual(values = c(
      "wgs" = "#636EFA",
      "EKG" = "#EF553B",
      "excap" = "#00CC96",
      "Test" = "#AB63FA"
    )) +
    theme_minimal()
```

```{r}
#| cache: true
r_ttt_sep_humanize <- py$ttt_sep_humanize
r_ttt_sep_sleep_humanize <- py$ttt_sep_sleep_humanize
r_slept_times_sep <- py$slept_times_sep
r_ttdata_sep <- py$ttdata_sep
r_ttp_sep_wgs <- py$ttp_sep_wgs
r_ttmd5_sep_humanize <- py$ttmd5_sep_humanize
r_tteffective_sep <- py$tteffective_sep
```
::: {.callout-caution}
#### [Attention]{#attention-sep}
* Total absolute time used for transferring files in September is [`r r_ttt_sep_humanize`]{style="font-family:Krona One;"}. 
In total [`r r_ttdata_sep`]{style="font-family:Krona One;"} data was transferred including
  [`r r_ttp_sep_wgs`]{style="font-family:Krona One;"} new wgs projects.

* The nsc-exporter will sleep 10 minutes before checking for new data to transfer. 
In September, nsc-exporter slept [`r r_slept_times_sep`]{style="font-family:Quicksand"} times, 
  totally [`r r_ttt_sep_sleep_humanize`]{style="font-family:Krona One;"}. [^2]

* Another type of time used is the md5sum checking by `s3cmd put` command 
which is not counted in the absolute transfer time. [^3]
The total time used for md5sum checking in September is [`r r_ttmd5_sep_humanize`]{style="font-family:Krona One;"}. 
:::

### October
<hr>

```{python} 
#| label: fig-timeseries-oct-trans-time
#| fig-cap: Transfer time of all files in October
py_df_oct = py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))]
px.scatter(
    py_df_oct,
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "time_gap", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

```{r} 
r_py_df_oct <- py$py_df_oct
```

```{python} 
#| label: fig-gap-oct
#| fig-cap: Idle time in October (transfer gaps)
px.scatter(
    py_df_oct,
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "seconds", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "time_gap": "Interval to next transfer start (seconds)"},
    title="transfer gaps (time difference between two transfer starts)",
    height=600,
)
```

```{python} 
import humanize
import datetime
#| cache: true
ttt_oct = sum(py_df_oct["seconds"])
ttmd5_oct = sum(py_df_oct["md5_check"])
ttdata_oct = humanize.naturalsize(sum(py_df_oct["bytes"]))
ttmd5_oct_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttmd5_oct))
ttt_oct_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttt_oct))
slept_times_oct = len(slept[(pd.to_datetime("2023-10-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-11-01")) & slept["small_sleep"]])
slept_tot_oct = slept_times_oct * 600
ttt_oct_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_oct))
ttp_oct = py_df_oct.groupby("project", as_index=False).count()
ttp_oct_wgs = len(ttp_oct[(ttp_oct["project"].apply(lambda x: x.startswith("wgs"))) & (ttp_oct["filename"]>2000)])
tteffective_oct = humanize.precisedelta(ttt_oct + slept_tot_oct + ttmd5_oct)
```

```{r} 
#| label: fig-md5-check-oct
#| fig-cap: Time used for md5sum check in October
#| fig-width: 12
library(reticulate)
r_py_df_oct %>% 
  mutate(r_datetime=as.POSIXct(datetime, format="%Y-%m-%d %H:%M:%S")) %>% 
    filter(!(project_type == "wgs" & md5_check == 0)) %>% 
      ggplot(aes(x = r_datetime, y=md5_check, color=project_type)) + 
      geom_point() + 
      scale_x_datetime(date_minor_breaks = "1 day") + 
      labs(
        x = "datetime",
        y = "md5sum check time (seconds)",
        title = "Time used for md5sum check"
      ) +
      theme(plot.title = element_text(hjust = 0.5)) +
      guides(color = guide_legend(title = "Project Type")) + 
      scale_color_manual(values = c(
        "wgs" = "#636EFA",
        "EKG" = "#EF553B",
        "excap" = "#00CC96",
        "Test" = "#AB63FA"
      )) +
      theme_minimal()
```

```{r}
#| cache: true
library(reticulate)
r_ttt_oct_humanize <- py$ttt_oct_humanize
r_ttt_oct_sleep_humanize <- py$ttt_oct_sleep_humanize
r_slept_times_oct <- py$slept_times_oct
r_ttdata_oct <- py$ttdata_oct
r_ttp_oct_wgs <- py$ttp_oct_wgs
r_ttmd5_oct_humanize <- py$ttmd5_oct_humanize
r_tteffective_oct <- py$tteffective_oct
```

::: {.callout-caution}
#### [Attention]{#attention-oct}
* Total absolute time used for transferring files in October is [`r r_ttt_oct_humanize`]{style="font-family:Krona One;"}.
In total [`r r_ttdata_oct`]{style="font-family:Krona One;"} data was transferred including 
  [`r r_ttp_oct_wgs`]{style="font-family:Krona One;"} new wgs projects.

* The nsc-exporter will sleep 10 minutes before checking for new data to transfer. In October, nsc-exporter slept [`r r_slept_times_oct`]{style="font-family:Quicksand"} times, totally [`r r_ttt_oct_sleep_humanize`]{style="font-family:Krona One;"}.

* The total time used for md5sum checking in October is [`r r_ttmd5_oct_humanize`]{style="font-family:Krona One;"}. 
:::

### November
<hr>

```{python} 
#| label: fig-timeseries-nov-trans-time
#| fig-cap: Transfer time of all files in November
py_df_nov = py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))]
px.scatter(
    py_df_nov,
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "time_gap", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

```{r} 
r_py_df_nov <- py$py_df_nov
```

```{python} 
#| label: fig-gap-nov
#| fig-cap: Idle time in November (transfer gaps)
px.scatter(
    py_df_nov,
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    hover_data=["size", "seconds", "file_extension", "speed_humanize"],
    labels={"datetime": "transfer start datetime", "time_gap": "Interval to next transfer start (seconds)"},
    title="transfer gaps (time difference between two transfer starts)",
    height=600,
)
```

```{python} 
import humanize
import datetime
#| cache: true
ttt_nov = sum(py_df_nov["seconds"])
ttmd5_nov = sum(py_df_nov["md5_check"])
ttdata_nov = humanize.naturalsize(sum(py_df_nov["bytes"]))
ttmd5_nov_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttmd5_nov))
ttt_nov_humanize_nov = humanize.precisedelta(datetime.timedelta(seconds=ttt_nov))
slept_times_nov = len(slept[(pd.to_datetime("2023-11-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-12-01")) & slept["small_sleep"]])
slept_tot_nov = slept_times_nov * 600
ttt_nov_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_nov))
ttp_nov = py_df_nov.groupby("project", as_index=False).count()
ttp_nov_wgs = len(ttp_nov[(ttp_nov["project"].apply(lambda x: x.startswith("wgs"))) & (ttp_nov["filename"]>2000)])
tteffective_nov = humanize.precisedelta(ttt_nov + slept_tot_nov + ttmd5_nov)
```

```{r} 
#| label: fig-md5-check-nov
#| fig-cap: Time used for md5sum check in November
#| fig-width: 12
library(reticulate)
r_py_df_nov %>% 
  mutate(r_datetime=as.POSIXct(datetime, format="%Y-%m-%d %H:%M:%S")) %>% 
    filter(!(project_type == "wgs" & md5_check == 0)) %>% 
    ggplot(aes(x = r_datetime, y=md5_check, color=project_type)) + 
    geom_point() + 
    scale_x_datetime(date_minor_breaks = "1 day") + 
    labs(x = "datetime", y = "md5sum check time (seconds)", title = "Time used for md5sum check") +
    theme(plot.title = element_text(hjust = 0.5)) +
    guides(color = guide_legend(title = "Project Type")) + 
    scale_color_manual(values = c(
      "wgs" = "#636EFA",
      "EKG" = "#EF553B",
      "excap" = "#00CC96",
      "Test" = "#AB63FA"
    )) +
    theme_minimal()
```

```{r}
#| cache: true
library(reticulate)
r_ttt_nov_humanize_nov <- py$ttt_nov_humanize_nov
r_ttt_nov_sleep_humanize <- py$ttt_nov_sleep_humanize
r_slept_times_nov <- py$slept_times_nov
r_ttdata_nov <- py$ttdata_nov
r_ttp_nov_wgs <- py$ttp_nov_wgs
r_ttmd5_nov_humanize <- py$ttmd5_nov_humanize
r_tteffective_nov <- py$tteffective_nov
```

::: {.callout-caution}
#### [Attention]{#attention-nov}
* Total absolute time used for transferring files in November is [`r r_ttt_nov_humanize_nov`]{style="font-family:Krona One;"}. 
In total [`r r_ttdata_nov`]{style="font-family:Krona One;"} data was transferred including
[`r r_ttp_nov_wgs`]{style="font-family:Krona One;"} new wgs projects.

* The nsc-exporter will sleep 10 minutes before checking for new data to transfer. In November, nsc-exporter slept [`r r_slept_times_nov`]{style="font-family:Quicksand"} times, totally [`r r_ttt_nov_sleep_humanize`]{style="font-family:Krona One;"}.

* The total time used for md5sum checking in November is [`r r_ttmd5_nov_humanize`]{style="font-family:Krona One;"}. 
:::

## Discussion


Obviously, with a 10Gbps switch connecting NSC and TSD, the current transfer speed is not optimal. There can be many
reasons for this, e.g. network components, software and protocol used to transfer data, the storage system I/O, CPU
performance o both ends etc.

### Do we transfer too many small files? {#sec-too-many-small-files}
<hr>


@fig-small-files-count show the number of small files and the number of large files using different boundaries.
@fig-small-files-tot-time shows that the time used to transfer small files is neglectable. 

```{python} 
#| label: fig-small-files-count
#| fig-cap: total number of small files vs large files
#| cache: false
boundary = [100000, 1000000, 10000000, 100000000, 1000000000]
small = []
large = []

for threshold in boundary:
    small.append(len(py_df[py_df["bytes"] < threshold]))
    large.append(len(py_df[py_df["bytes"] >= threshold]))

filecount_df = pd.DataFrame(
    {
        "boundary": map(
            lambda x: "<" + r.humanReadable(x, standard="SI", digits=0),
            boundary,
        ),
        "small": small,
        "large": large,
    },
)

px.bar(
    filecount_df,
    x="boundary",
    y=["small", "large"],
    labels={
        'boundary': 'how small is small?',
        'value': 'file count',
        'variable': 'category',
    },
    title="Number Of Small Files vs Large Files",
    color_discrete_map={"small": "seagreen", "large": "orangered"},
)
```

```{python} 
#| label: fig-small-files-tot-time
#| fig-cap: total time used for transferring small files
#| cache: false
small_tot_time = []
large_tot_time = []
for threshold in boundary:
    small_tot_time.append(sum(py_df[py_df["bytes"] < threshold]["seconds"]))
    large_tot_time.append(sum(py_df[py_df["bytes"] >= threshold]["seconds"]))
time_df = pd.DataFrame(
    {
        "boundary": map(
            lambda x: "<" + r.humanReadable(x, standard="SI", digits=0),
            boundary,
        ),
        "small": small_tot_time,
        "large": large_tot_time,
    },
)
px.bar(
    time_df,
    x="boundary",
    y=["small", "large"],
    labels={
        'boundary': 'how small is small?',
        'value': 'total time used (seconds)',
        'variable': 'category',
    },
    title="Total Time Used To Transfer Small Files vs Large Files",
    color_discrete_map={"small": "seagreen", "large": "orangered"},
)
```

::: {.callout-tip collapse="true"}
#### Total time (seconds) used to transfer small vs larger files (raw data)

```{r} 
#| output: asis
library(reticulate)
library(jtools)
cat(md_table(
    py$time_df,
    align = "lrrr",
    format="markdown",
    col.names = c("threshold", "small files total time(s)", "large files total time(s)")
))
```
:::
###  Possibility Of an additional 48-sample run each week?
<hr>

* The nsc-exporter is idle for quite a portion of the time @sec-idle-time.
    + [September](#attention-sep)'s effective transfer time[^effectivetime] is [`r r_tteffective_sep`]{style="font-family:Krona One;"}
      to transfer [`r r_ttdata_sep`]{style="font-family:Krona One;"} data
      including that of [`r r_ttp_sep_wgs`]{style="font-family:Krona One"} wgs projects.
    + [October](#attention-oct)'s effective transfer time is [`r r_tteffective_oct`]{style="font-family:Krona One;"} 
      to transfer [`r r_ttdata_oct`]{style="font-family:Krona One;"} data
      including that of [`r r_ttp_oct_wgs`]{style="font-family:Krona One"} wgs projects.
    + [November](#attention-nov)'s effective transfer time is [`r r_tteffective_nov`]{style="font-family:Krona One;"} 
      to transfer [`r r_ttdata_nov`]{style="font-family:Krona One;"} data
      including that of [`r r_ttp_nov_wgs`]{style="font-family:Krona One"} wgs projects.
* The maximum transfer speed is reached around 200 MB file size. @fig-correlation-filesize-speed-200MB
  This interestingly is the configured chunk size of `s3cmd` which is the tool used by nsc-exporter for data transfer.
  We might want to increase the chunk size to improve the transfer speed of large files?

* The current transfer speed is not optimal considering the 10Gbps switch connecting NSC and TSD. We need to investigate the reason for the low transfer speed.


### Calculator
<hr>

```{ojs}
//| echo: false
//| panel: sidebar
md`Assuming continuous transfer at speed (**MB/s**):`
viewof speed = Inputs.range([1, 500], {value: 80, step: 1})
```
```{ojs}
//| echo: false
import {textcolor} from "@observablehq/text-color-annotations-in-markdown"

formatBytes = function formatBytes(bytes,decimals) {
   if(bytes == 0) return '0 Bytes';
   var k = 1024,
       dm = decimals || 2,
       sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'],
       i = Math.floor(Math.log(bytes) / Math.log(k));
   return parseFloat((bytes / Math.pow(k, i)).toFixed(dm)) + ' ' + sizes[i];
}

volume_hour= formatBytes(speed*60*60*1000*1000, 1)
volume_day= formatBytes(speed*24*60*60*1000*1000 , 1)
volume_week = formatBytes(speed*7*24*60*60*1000*1000, 1)
volume_month = formatBytes(speed*30*24*60*60*1000*1000, 1)
md`${textcolor(volume_hour, 'MediumSeaGreen')} data can be transferred per **hour**.`
md`${textcolor(volume_day, 'SlateBlue')} data can be transferred per **day**.`
md`${textcolor(volume_week, 'Chocolate')} data can be transferred per **week**.`
md`${textcolor(volume_month, 'Crimson')} data can be transferred per **month**.`
```

### Debugging

1. **Does [`tacl`](https://github.com/unioslo/tsd-api-client) transfer data at a higher speed?**

The [`tacl`](https://github.com/unioslo/tsd-api-client) tool transfers speed is actually lower than `s3cmd`.
The `tacl` tool transfers data at around 65 MB/s.

2. **CPU performance on NSC side**

CPU usage on NSC side (sleipnir) is high during data transfer. It can reach 90% of a single core.

3. **Does increasing the chunk size of `s3cmd` improve the transfer speed?**

The 200MB chunk size is almost the optimal size for the current setup. 2500MB chunk size is slightly 
better (+10%) than 200MB chunk size for very large files.

4. **Does parallelizing the transfer improve the transfer speed?**

With two parallel `nsc-exporter` processes, we can achieve around **1.6x - 1.8x** the transfer speed of a single process.


## Conclusion

* We can run `4 x 48` or `2 x 48 + 1 x 96` samples per week. Then the allowance for downtime is reduced to 2 days per week.
* 200MB chunk size is almost the optimal size for the current setup.
* With 2 parallel `nsc-exporter` processes, we can achieve around **1.6x - 1.8x** the transfer speed of a single
  process with current setup.
* We are waiting or the investigation of the low transfer speed on TSD side, e.g. TCP Proxy, MinIO performance,
     storage I/O etc.

# Data storage
<hr>

WGS produces large amount of data. The data storage capacity is critical for the upscaling. 


## NSC


On NSC side, the data is stored in on **boston** at `/boston/diag`. 
Boston has a total capacity of 1.5 PB, and the usable capacity is 320TB at the moment.


## TSD


On TSD side, the data is stored in `/cluster/projects/p22`. 
The total capacity is 1.8 PB, and the usable capacity is 1.2 PB at the moment.



# Pipeline capacity (Illumina DRAGEN)
<hr>

Illunima DRAGEN is a bioinformatics pipeline server that can be used to process WGS data. 
It takes around 1 hours to process a 30x WGS sample.

To be extended ...


# Discussion
<hr>

To be added...


# Conclusion
<hr>

To be added... 


[^1]: The nsc-exporter log and sequencer overview html files are very small files and do not belong to any projects.
      They are always transferred in a very short time.
      They will not affect the transfer speed of other files. 
      Therefore, they are ignored for simplicity.
[^2]: Data comes in a continuous manner. 
      The nsc-exporter normally takes a snapshot of new data and transfer it. 
      It then sleeps for 10 minutes before checking for new data. 
      The slept times counted here are where next sleep is more than the sleeping interval (10 minutes) later,
        signifying that new data comes right after the sleep. 
      This is contrary to long idle time where no new data comes after the sleep.
[^3]: To make sure files are transferred intact, `s3cmd put` checks the *md5sum* of the files. 
      This takes time and is not reported as transfer time by `s3cmd`
      The md5sum check is done before starting the transfer. 
      We estimate the md5sum check time by subtracting the transfer time from the time gap between two transfers.
      The md5sum check time is usually less than 10 minutes, so a gap larger than 10 minutes is not considered as md5sum check time.
[^md5sum]: Skipped wgs records where md5sum check time equals/close to zero, so project types with small files/folders such as EKG are shown.
[^effectivetime]: Effective transfer time is calculated as "absolute transfer time" + "sleep time between 2 transfers" + "md5sum check time".
