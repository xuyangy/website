---
title: "WGS Upscaling - IT & Bioinformatics Evaluation"
title-block-banner: true
description: "Data transfer, Data storage, Bioinformatics pipeline capacity"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx"
      url: "http://www.robotgenome.com"
      role: "collect data"
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
#     fig-cap-location: top
date: last-modified
format: 
    html:
        theme: Journal
        toc: true
        toc-depth: 4
        number-sections: true
        number-depth: 4
        float: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
execute: 
    cache: true
    echo: false
    message: false
    warning: false
---

# Background
GDx at OUSAMG is planning to upscale the WGS production to `4 x 48` samples or `2 x 48 + 1 x 96` samples per week. 

This document evaluates the possible bottlenecks of IT & bioinformatics pipelines in following areas:

1. Data transfer speed
2. Data storage
3. Pipeline capacity (Illumina DRAGEN)

# IT && Bioinformatics


## Data transfer speed

```{r}
#| echo: false
#| message: false
#| warning: false
recognize_bandwidth <- function(bandwidth_string) {
  # Use regular expression to extract numeric part and unit part
  match_result <- regexec("^([0-9.]+)\\s*([KMGT]?B)(/s)?$", bandwidth_string)

  if (match_result[[1]][1] == -1) {
    stop("Invalid bandwidth string format")
  }

  # Extract numeric and unit parts
  numeric_part <- as.numeric(regmatches(bandwidth_string, match_result)[[1]][2])
  unit_part <- regmatches(bandwidth_string, match_result)[[1]][3]

  # Convert to bytes per second
  bytes_per_second <- switch(unit_part,
    "B" = numeric_part,
    "KB" = numeric_part * 1000,
    "MB" = numeric_part * 1000^2,
    "GB" = numeric_part * 1000^3,
    stop("Invalid unit")                                     
  )                                                          
                                                             
  return(bytes_per_second)                                   
}                                                            
                                                             
transform_bandwidth <- function(bandwidth) {
  return(lapply(bandwidth, recognize_bandwidth))
}

# load data from csv file
library(readr)               
library(dplyr)               
data <- read_csv("data.csv", col_names = TRUE, col_types = "cccddc")
data <- mutate(data, speed = sapply(speed, recognize_bandwidth))
```



### Collect Historical Data Transfer Records

To evaluate the data transfer speed, we collected the transfer time of all files that were transferred from NSC to TSD [`r strptime(min(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} and [`r strptime(max(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"}.

::: {.callout-important}
## The nsc-exporter log and sequencer overview html files were ignored for simplicity.
:::

::: {.callout-tip collapse="true"}


## examples of collected data


```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(stringr)
for (extension in c(".bam", ".fastq.gz", ".vcf", ".sample", ".pdf")) {
  print(t(data |> filter(str_ends(data$filename, extension)) |> slice_sample(n = 1)))
}
```

:::


### Data Overview
::: {.panel-tabset}
#### File Size
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(gdata)
setNames(as.data.frame(sapply(summary(data$bytes), humanReadable)), "filesize")
```
:::
::: {.column width="70%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
ggplot(
    data,
    aes(x = bytes)
) +
    geom_histogram(fill = "#fc03d3", bins = 200) +
    labs(x = "bytes", y = "count", title = "filesize") + theme_minimal()
ggplot(
    data,
    aes(x = bytes)
) +
    geom_histogram(fill = "#fc03d3", bins = 200) +
    labs(x = "bytes", y = "count (log)", title = "filesize (log)") + scale_y_continuous(trans='log2') + theme_minimal()
```
:::
::::
#### Transfer Speed
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
setNames(as.data.frame(sapply(summary(data$speed), humanReadable)), "speed(/s)")
```
:::
::: {.column width="70%"}
```{r} 
library(ggplot2)
ggplot(
    data,
    aes(x = speed)
) + 
    geom_histogram(bins=200, fill="#0bd440") + 
    labs(x = "bytes/s", y = "count", title = "transfer speed") + theme_minimal()
ggplot(
    data,
    aes(x = speed)
) + 
    geom_histogram(bins=200, fill="#0bd440") + 
    labs(x = "bytes/s", y = "count (log)", title = "transfer speed (log)") + scale_y_continuous(trans='log2') + theme_minimal()
```
:::
::::
#### Transfer Time
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
summary(data %>% select(seconds))
```
:::
::: {.column width="70%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
ggplot(
    data,
    aes(x = seconds)
) + 
    geom_histogram(bins=200, fill="#e3571b") + 
    labs(x = "seconds", y = "count", title = "transfer time") + theme_minimal()
ggplot(
    data,
    aes(x = seconds)
) + 
    geom_histogram(bins=200, fill="#e3571b") + 
    labs(x = "seconds", y = "count (log)", title = "transfer time (log)") + scale_y_continuous(trans='log2') + theme_minimal()
```
:::
::::
:::

### Corelation Between File Size And Transfer Time And Transfer Speed


<!-- read data from csv file -->
```{python}
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| include: false
def deHumanReadable(bandwidth_string):
    match_result = re.match(r'^([0-9.]+)\s*([KMGT]?B)(/s)?$', bandwidth_string)

    if match_result is None:
        raise ValueError("Invalid bandwidth string format")

    numeric_part = float(match_result.group(1))
    unit_part = match_result.group(2)
    bytes_per_second = {
        "B": numeric_part,
        "KB": numeric_part * 1000,
        "MB": numeric_part * 1000**2,
        "GB": numeric_part * 1000**3
    }.get(unit_part, ValueError("Invalid unit"))

    return bytes_per_second

```

#### Transfer speed and time VS file size (all files)


```{python} 
#| echo: false
#| cache: false
import pandas as pd
from datetime import datetime
import re
import plotly.express as px
py_df = pd.read_csv("data.csv", 
                    sep=",", 
                    parse_dates=["datetime"],
                    date_format="%Y-%m-%d %H:%M:%S.%f")
py_df["speed"] = py_df["speed"].apply(deHumanReadable)
py_df["project_type"] = py_df["project"].apply(lambda x: re.match(r"[a-zA-Z]+", x).group())
fig = px.scatter(py_df, x="bytes", y="speed", opacity=0.3, color="project_type", height=500)
fig.show()

fig_time = px.scatter(py_df, x="bytes", y="seconds", opacity=0.3, color="project_type", height=500)
fig_time.show()
```

#### Transfer speed and time VS file size (small files)

```{python} 
#| echo: false
import plotly.express as px
small_files = py_df[py_df["bytes"] < 300000]
fig_sf = px.scatter(small_files, x="bytes", y="speed", opacity=0.3, color="project_type", height=500)
fig_sf.show()

fig_sf_time = px.scatter(small_files, x="bytes", y="seconds", opacity=0.3, color="project_type", height=500)
fig_sf_time.show()
```


#### [ Maximum transfer reached around <u>200MB</u> file size? ]{style="color:orange;"}

```{python} 
#| echo: false
import plotly.express as px
small_files = py_df[py_df["bytes"] < 500000000]
fig_200MB = px.scatter(small_files, x="bytes", y="speed", opacity=0.3, color="project_type", height=500)
fig_200MB.show()
```

### Idle Time

How much time when nsc-exporter is not transferring files?
```{python} 
#| cache: false
py_df['datetime_shifted']= py_df["datetime"].shift().fillna(pd.to_datetime(py_df["datetime"][0]))
py_df['time_spent'] = (pd.to_datetime(py_df['datetime']) - pd.to_datetime(py_df['datetime_shifted']))/pd.Timedelta(1, "S")
```
#### September
```{python} 
px.scatter(py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", labels={"x": "transfer start", "y": "transfer time"}, title="transfer records(per file)", height=600)
print("logarithmic time")
px.scatter(py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", log_y=True, labels=["x", "y"], title="transfer records(per file)", height=600)
```

#### October
```{python} 
px.scatter(py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", labels={"x": "transfer start", "y": "transfer time"}, title="transfer records(per file)", height=600)
print("logarithmic time")
px.scatter(py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", log_y=True, labels=["x", "y"], title="transfer records(per file)", height=600)
```

#### November
```{python} 
px.scatter(py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", labels={"x": "transfer start", "y": "transfer time"}, title="transfer records(per file)", height=600)
print("logarithmic time")
px.scatter(py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))], x="datetime", y="time_spent", opacity=0.3, color="project", symbol="project_type", log_y=True, labels=["x", "y"], title="transfer records(per file)", height=600)
```

### Discussion

* The nsc-exporter is idle for quite a portion of the time.
    + Quite long idle time in September observed. 
    + Almost 12 wgs projects were transferred in November. 
* The maximum transfer speed is reached around 200 MB file size. This is the configured chunk size of s3cmd which is the tool used for data transfer. We might want to increase the chunk size to improve the transfer speed?
* The current transfer speed is not optimal considering the 10Gbps switch connecting NSC and TSD. We need to investigate the reason for the low transfer speed.


### Conclusion

* We might be able to run `4 x 48` or `2 x 48 + 1 x 96` samples per week with the current transfer speed. However, we will reach maximum capicity of data transfer.
* If we can increase the transfer speed, e.g. reaching 200MB/s, we can easily double current production capacity.

## Data storage

WGS produces large amount of data. The data storage capacity is critical for the upscaling. 


### NSC
On NSC side, the data is stored in on **boston** at `/boston/diag`. Boston has a total capacity of 1.5 PB, and the usable capacity is 1.2 at the moment.


### TSD

On TSD side, the data is stored in `/cluster/projects/p22`. The total capacity is 1.8 PB, and the usable capacity is 1.2 PB at the moment.



## Pipeline capacity (Illumina DRAGEN)

Illunima DRAGEN is a bioinformatics pipeline server that can be used to process WGS data. It takes around 1 hours to process a 30x WGS sample.


# Discussion
To be addded...


# Conclusion

To be added...
