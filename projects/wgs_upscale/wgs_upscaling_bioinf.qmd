---
title: "WGS Upscaling - IT & Bioinformatics Evaluation"
title-block-banner: true
description: "Data transfer, Data storage, Bioinformatics pipeline capacity"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx"
      url: "http://www.robotgenome.com"
      role: "collect data"
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
#     fig-cap-location: top
date: last-modified
format: 
    html:
        theme: Journal
        toc: true
        toc-depth: 4
        # toc-expand: 3
        number-sections: true
        number-depth: 4
        float: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
            <link href="https://fonts.googleapis.com/css2?family=Krona+One&display=swap" rel="stylesheet">
execute: 
    cache: true
    echo: false
    message: false
    warning: false
---

# Background
GDx at OUSAMG is planning to upscale the WGS production to `4 x 48` samples or `2 x 48 + 1 x 96` samples per week. 

This document evaluates the possible bottlenecks of IT & bioinformatics pipelines in following areas:

1. Data transfer speed
2. Data storage
3. Pipeline capacity (Illumina DRAGEN)


# IT && Bioinformatics


## Data transfer speed

```{r}
#| echo: false
#| message: false
#| warning: false
recognize_bandwidth <- function(bandwidth_string) {
  # Use regular expression to extract numeric part and unit part
  match_result <- regexec("^([0-9.]+)\\s*([KMGT]?B)(/s)?$", bandwidth_string)

  if (match_result[[1]][1] == -1) {
    stop("Invalid bandwidth string format")
  }

  # Extract numeric and unit parts
  numeric_part <- as.numeric(regmatches(bandwidth_string, match_result)[[1]][2])
  unit_part <- regmatches(bandwidth_string, match_result)[[1]][3]

  # Convert to bytes per second
  bytes_per_second <- switch(unit_part,
    "B" = numeric_part,
    "KB" = numeric_part * 1000,
    "MB" = numeric_part * 1000^2,
    "GB" = numeric_part * 1000^3,
    stop("Invalid unit")                                     
  )                                                          
                                                             
  return(bytes_per_second)                                   
}                                                            
                                                             
transform_bandwidth <- function(bandwidth) {
  return(lapply(bandwidth, recognize_bandwidth))
}

# load data from csv file
library(readr)               
library(dplyr)               
data <- read_csv("data.csv", col_names = TRUE, col_types = "cccddc")
data <- mutate(data, speed = sapply(speed, recognize_bandwidth))
```



### Collect Historical Data Transfer Records

To evaluate the data transfer speed, we collected the transfer time of all files that were transferred from NSC to TSD between [`r strptime(min(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} and [`r strptime(max(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} from the nsc-exporter. The nsc-exporter is the tool that is used to transfer data from NSC to TSD.


::: {.callout-tip collapse="true"}

## examples of collected data


```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(stringr)
for (extension in c(".bam", ".fastq.gz", ".vcf", ".sample", ".pdf")) {
  print(t(data |> filter(str_ends(data$filename, extension)) |> slice_sample(n = 1)))
}
```

:::

::: {.callout-note}
## The nsc-exporter log files and the sequencer overview html files were ignored for simplicity. [^1]


:::
### Data Overview
::: {.panel-tabset}
#### File Size
```{r}
#| cache: false
library(gdata)
```
The file size of the collected data ranges from [`r humanReadable(min(data$bytes))`]{style="font-family:Quicksand;"} to [`r humanReadable(max(data$bytes))`]{style="font-family:Quicksand;"}. The average file size is [`r humanReadable(mean(data$bytes))`]{style="font-family:Quicksand;"}. The median file size is [`r humanReadable(median(data$bytes))`]{style="font-family:Quicksand;"}. The standard deviation is [`r humanReadable(sd(data$bytes))`]{style="font-family:Quicksand;"}.

:::: {.columns}
::: {.column width="30%"}
```{r} 
setNames(as.data.frame(sapply(summary(data$bytes), humanReadable)), "filesize")
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
library(ggplot2)
ggplot(
    data,
    aes(x = bytes)
) +
    geom_histogram(fill = "#fc03d3", bins = 200) +
    labs(x = "file size (bytes)", y = "count", title = "File Size") + theme_minimal()
ggplot(
    data,
    aes(x = bytes)
) +
geom_histogram(fill = "#fc03d3", bins = 200) +
labs(
    x = "file size (bytes)",
    y = "count (log10)",
    title = "File Size (log10)"
) + 
scale_y_continuous(trans='log10') + theme_minimal()
```
:::
::::
#### Transfer Speed
The transfer speed ranges from [`r humanReadable(min(data$speed))`]{style="font-family:Quicksand;"} to [`r humanReadable(max(data$speed))`]{style="font-family:Quicksand;"}. The average transfer speed is [`r humanReadable(mean(data$speed))`]{style="font-family:Quicksand;"}. The median transfer speed is [`r humanReadable(median(data$speed))`]{style="font-family:Quicksand;"}. The standard deviation is [`r humanReadable(sd(data$speed))`]{style="font-family:Quicksand;"}.

:::: {.columns}
::: {.column width="30%"}
```{r} 
setNames(as.data.frame(sapply(summary(data$speed), humanReadable)), "speed(/s)")
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
ggplot(
  data,
  aes(x = speed)
) + 
  geom_histogram(bins = 200, fill = "#0bd440") + 
  labs(x = "bytes/s", y = "count", title = "Transfer Speed") + 
  theme_minimal()

ggplot(
    data,
    aes(x = speed)
) + 
  geom_histogram(bins = 200, fill = "#0bd440") + 
  labs(
    x = "bytes/s",
    y = "count (log10)",
    title = "Transfer Speed (log10)"
  ) + 
  scale_y_continuous(trans = 'log10') +
  theme_minimal()
```
:::
::::
#### Transfer Time
The transfer time ranges from [`r round(min(data$seconds), 1)`]{style="font-family:Quicksand;"} seconds to [`r round(max(data$seconds), 1)`]{style="font-family:Quicksand;"} seconds. The average transfer time is [`r round(mean(data$seconds),1)`]{style="font-family:Quicksand;"} seconds. The median transfer time is [`r round(median(data$seconds),1)`]{style="font-family:Quicksand;"} seconds. The standard deviation is [`r round(sd(data$seconds),1)`]{style="font-family:Quicksand;"} seconds.

:::: {.columns}
::: {.column width="30%"}
```{r} 
summary(data %>% select(seconds))
```
:::
::: {.column width="70%"}
```{r} 
#| fig-height: 3
ggplot(
  data,
  aes(x = seconds)
) + 
  geom_histogram(bins = 200, fill = "#e3571b") + 
  labs(x = "transfer time(per file)", y = "count", title = "Transfer Time") + theme_minimal()

ggplot(
  data,
  aes(x = seconds)
) + 
  geom_histogram(bins = 200, fill = "#e3571b") + 
  labs(x = "transfer time(per file)", y = "count (log10)", title = "Transfer Time (log10)") + 
  scale_y_continuous(trans = 'log10') + 
  theme_minimal()
```
:::
::::
:::

### Correlation Between File Size And Transfer Time And Transfer Speed


<!-- read data from csv file -->
```{python}
#| cache: false
#| include: false
def deHumanReadable(bandwidth_string):
    match_result = re.match(r'^([0-9.]+)\s*([KMGT]?B)(/s)?$', bandwidth_string)

    if match_result is None:
        raise ValueError("Invalid bandwidth string format")

    numeric_part = float(match_result.group(1))
    unit_part = match_result.group(2)
    bytes_per_second = {
        "B": numeric_part,
        "KB": numeric_part * 1000,
        "MB": numeric_part * 1000**2,
        "GB": numeric_part * 1000**3
    }.get(unit_part, ValueError("Invalid unit"))

    return bytes_per_second

```

#### Transfer speed and time VS file size (all files)


Small files have lower transfer speed. A good transfer speed around 80 MB/s is achieved for files larger than 30 GB.
```{python} 
#| label: fig-correlation-filesize-speed-all
#| fig-cap: Transfer speed VS file size (all files)
#| cache: false
import pandas as pd
from datetime import datetime
import re
import plotly.express as px
py_df = pd.read_csv("data.csv", 
                    sep=",", 
                    parse_dates=["datetime"],
                    date_format="%Y-%m-%d %H:%M:%S.%f")
py_df["speed"] = py_df["speed"].apply(deHumanReadable)
py_df["project_type"] = py_df["project"].apply(lambda x: re.match(r"[a-zA-Z]+", x).group())
fig = px.scatter(
    py_df,
    x="bytes",
    y="speed",
    labels={"bytes": "file size (bytes)", "speed": "transfer speed"},
    opacity=0.3,
    color="project_type",
    height=500,
)
fig.show()
```

```{r}
#| label: fig-correlation-filesize-time-all
#| fig-cap: Transfer time VS file size (all files)
#| cache: false
#| fig-width: 10
#| fig-height: 4
library(reticulate)
library(ggplot2)

ggplot(py$py_df, aes(x = bytes, y = seconds, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer time (seconds)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  guides(color = guide_legend(title = "Project Type")) + 
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
```
#### Transfer speed and time VS file size (small files)


Although the transfer speed of small files are very low; the transfer time is usually very short. So small files are not the bottleneck of the data transfer.

```{r}
#| label: fig-correlation-filesize-speed-small
#| fig-cap: Transfer speed VS file size (small files)
#| fig-width: 10
#| fig-height: 4
#| cache: true
small_files <- py$py_df[py$py_df$bytes < 300000, ]
fig_sf <- ggplot(small_files, aes(x = bytes, y = speed, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer speed") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold")) +
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
print(fig_sf)
```

```{r}
#| label: fig-correlation-filesize-time-small
#| fig-cap: Transfer time VS file size (small files)
#| fig-width: 10
#| fig-height: 4

ggplot(small_files, aes(x = bytes, y = seconds, color = project_type)) +
  geom_point(alpha = 0.3) +
  labs(x = "file size (bytes)", y = "transfer time (seconds)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = c(
    "wgs" = "#636EFA",
    "EKG" = "#EF553B",
    "excap" = "#00CC96",
    "Test" = "#AB63FA"
  ))
```


#### [ Maximum transfer reached around <u>200MB</u> file size? ]{style="color:orange;"}

Small files have lower transfer speed. Large files have higher transfer speed. But it looks like best transfer speed is observed for files with sizearound 200 MB file size.

```{python} 
#| label: fig-correlation-filesize-speed-200MB
#| fig-cap: maximum transfer speed reached around 200MB file size
import plotly.express as px
small_files = py_df[py_df["bytes"] < 500000000]
fig_200MB = px.scatter(
    small_files,
    x="bytes",
    y="speed",
    labels={"bytes": "file size (bytes)", "speed": "transfer speed"},
    opacity=0.3,
    color="project_type",
    height=500,
)
fig_200MB.show()
```

### Idle Time

To evaluate whether there is capacity for upscaling, we need to know the idle time of the nsc-exporter. The nsc-exporter is idle when it is not transferring data.  

All transfer records are plotted with starting time of each transfer on x-axis and the time used to finished the transfer on y-axis. The gaps represnts idle periods of nsc-exporter. The color represents projects, *e.g.* wgs123, EKG20230901 *etc.*. The shape represents project type, *e.g.* wgs, EKG *etc.* You can turn off a project by clicking it in the legend to the right of the figure.  

For easier visualization, the data is grouped in months.

```{python} 
#| cache: false
#| include: false
py_df['datetime_shifted']= py_df["datetime"].shift(1)
py_df['time_gap'] = (pd.to_datetime(py_df['datetime']) - pd.to_datetime(py_df['datetime_shifted']))[1:].reset_index(drop=True)/pd.Timedelta(1, "S")
```
#### September
```{python} 
#| label: fig-timeseries-sep-trans-time
#| fig-cap: Transfer time of all files transferred in September
px.scatter(
    py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))],
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

```{python} 
#| label: fig-gap-sep
#| fig-cap: Idle time in September (transfer gaps)
px.scatter(
    py_df[(pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))],
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    log_y=False,
    labels={"datetime": "transfer start datetime", "time_gap": "Interval to next transfer start (seconds)"},
    title="transfer gaps (time difference between two transfer starts)",
    height=600,
)
```

```{python} 
import humanize
import datetime
#| cache: true
slept = pd.read_csv('sleeping_time.csv', parse_dates=['sleep_at'])
ttt_sep = sum(
    py_df[( pd.to_datetime("2023-09-01") <= pd.to_datetime(py_df["datetime"]) ) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-10-01"))]["seconds"],
)
ttt_sep_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttt_sep))
slept_times_sep = len(slept[(pd.to_datetime("2023-09-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-10-01")) & slept["small_sleep"]])
slept_tot_sep = slept_times_sep * 600
ttt_sep_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_sep))
```

```{r}
#| cache: true
r_ttt_sep_humanize <- py$ttt_sep_humanize
r_ttt_sep_sleep_humanize <- py$ttt_sep_sleep_humanize
r_slept_times_sep <- py$slept_times_sep
```
::: {.callout-caution}
##### absolute transfer time
Total absolute time used for transferring files in September is [`r r_ttt_sep_humanize`]{style="font-family:Krona One;"}.
:::

The nsc-exporter will sleep 10 minutes before checking for new data to transfer. In September, nsc-exporter slept [`r r_slept_times_sep`]{style="font-family:Quicksand"} times, totally [`r r_ttt_sep_sleep_humanize`]{style="font-family:Quicksand;"}. [^2]

Another type of time used is the md5sum checking by `s3cmd put` command. [^3]


#### October
```{python} 
#| label: fig-timeseries-oct-trans-time
#| fig-cap: All transfers in October in time order
px.scatter(
    py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))],
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

```{python} 
#| label: fig-gap-oct
#| fig-cap: Idle time in October
px.scatter(
    py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))],
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    labels={"datetime": "transfer start datetime", "time_gap": "time used (seconds)"},
    title="transfer records(per file)",
    height=600,
)
```

```{python} 
import humanize
import datetime
#| cache: true
ttt_oct = sum(py_df[(pd.to_datetime("2023-10-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-11-01"))]["seconds"])
ttt_oct_humanize = humanize.precisedelta(datetime.timedelta(seconds=ttt_oct))
slept_times_oct = len(slept[(pd.to_datetime("2023-10-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-11-01")) & slept["small_sleep"]])
slept_tot_oct = slept_times_oct * 600
ttt_oct_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_oct))
```

```{r}
#| cache: true
library(reticulate)
r_ttt_oct_humanize <- py$ttt_oct_humanize
r_ttt_oct_sleep_humanize <- py$ttt_oct_sleep_humanize
r_slept_times_oct <- py$slept_times_oct
```

::: {.callout-caution}
##### absolute transfer time
Total absolute time used for transferring files in October is [`r r_ttt_oct_humanize`]{style="font-family:Krona One;"}.
:::

The nsc-exporter will sleep 10 minutes before checking for new data to transfer. In October, nsc-exporter slept [`r r_slept_times_oct`]{style="font-family:Quicksand"} times, totally [`r r_ttt_oct_sleep_humanize`]{style="font-family:Quicksand;"}.


#### November
```{python} 
#| label: fig-timeseries-nov-trans-time
#| fig-cap: All transfers in November in time order
px.scatter(
    py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))],
    x="datetime",
    y="seconds",
    opacity=0.3,
    color="project",
    symbol="project_type",
    labels={"datetime": "transfer start datetime", "seconds": "time used (seconds)"},
    title="transfer time (per file)",
    height=600,
)
```

```{python} 
#| label: fig-gap-nov
#| fig-cap: Idle time in November
px.scatter(
    py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))],
    x="datetime",
    y="time_gap",
    opacity=0.3,
    color="project",
    symbol="project_type",
    labels={"datetime": "transfer start datetime", "time_gap": "time used (seconds)"},
    title="transfer records(per file)",
    height=600,
)
```

```{python} 
import humanize
import datetime
#| cache: true
ttt_nov = sum(py_df[(pd.to_datetime("2023-11-01") <= pd.to_datetime(py_df["datetime"])) & (pd.to_datetime(py_df["datetime"]) < pd.to_datetime("2023-12-01"))]["seconds"])
ttt_nov_humanize_nov = humanize.precisedelta(datetime.timedelta(seconds=ttt_nov))
slept_times_nov = len(slept[(pd.to_datetime("2023-11-01") <= pd.to_datetime(slept["sleep_at"])) & (pd.to_datetime(slept["sleep_at"]) < pd.to_datetime("2023-12-01")) & slept["small_sleep"]])
slept_tot_nov = slept_times_nov * 600
ttt_nov_sleep_humanize = humanize.precisedelta(datetime.timedelta(seconds=slept_tot_nov))
```

```{r}
#| cache: true
library(reticulate)
r_ttt_nov_humanize_nov <- py$ttt_nov_humanize_nov
r_ttt_nov_sleep_humanize <- py$ttt_nov_sleep_humanize
r_slept_times_nov <- py$slept_times_nov
```

::: {.callout-caution}
##### absolute transfer time
Total absolute time used for transferring files in November is [`r r_ttt_nov_humanize_nov`]{style="font-family:Krona One;"}.
:::

The nsc-exporter will sleep 10 minutes before checking for new data to transfer. In November, nsc-exporter slept [`r r_slept_times_nov`]{style="font-family:Quicksand"} times, totally [`r r_ttt_nov_sleep_humanize`]{style="font-family:Quicksand;"}.

### Discussion

#### Do we transfer too many small files?
```{python} 
#| label: fig-small-files-count
#| fig-cap: total number of small files vs large files
#| cache: false
boundary = [100000, 1000000, 10000000, 100000000, 1000000000]
small = []
large = []

for threshold in boundary:
    small.append(len(py_df[py_df["bytes"] < threshold]))
    large.append(len(py_df[py_df["bytes"] >= threshold]))

filecount_df = pd.DataFrame(
    {
        "boundary": map(
            lambda x: "<" + r.humanReadable(x, standard="SI", digits=0),
            boundary,
        ),
        "small": small,
        "large": large,
    },
)

px.bar(
    filecount_df,
    x="boundary",
    y=["small", "large"],
    labels={
        'boundary': 'how small is small?',
        'value': 'file count',
        'variable': 'category',
    },
    title="Number Of Small Files vs Large Files",
    color_discrete_map={"small": "seagreen", "large": "orangered"},
)
```

```{python} 
#| label: fig-small-files-tot-time
#| fig-cap: total time used for transferring small files
#| cache: false
small_tot_time = []
large_tot_time = []
for threshold in boundary:
    small_tot_time.append(sum(py_df[py_df["bytes"] < threshold]["seconds"]))
    large_tot_time.append(sum(py_df[py_df["bytes"] >= threshold]["seconds"]))
time_df = pd.DataFrame(
    {
        "boundary": map(
            lambda x: "<" + r.humanReadable(x, standard="SI", digits=0),
            boundary,
        ),
        "small": small_tot_time,
        "large": large_tot_time,
    },
)
px.bar(
    time_df,
    x="boundary",
    y=["small", "large"],
    labels={
        'boundary': 'how small is small?',
        'value': 'total time used (seconds)',
        'variable': 'category',
    },
    title="Total Time Used To Transfer Small Files vs Large Files",
    color_discrete_map={"small": "seagreen", "large": "orangered"},
)
```

::: {.callout-tip collapse="true"}
##### Total time (seconds) used to transfer small vs larger files (raw data)

```{r} 
#| output: asis
library(reticulate)
library(jtools)
cat(md_table(
    py$time_df,
    align = 'lrrr',
    format="markdown",
    col.names = c("threshold", "small files", "large files")
))
```
:::
####  Possibility Of One More 48-sample Run Per Week

* The nsc-exporter is idle for quite a portion of the time.
    + Quite long idle time in September observed @fig-timeseries-sep-trans-time.
    + Almost 12 wgs projects were transferred in November.
* The maximum transfer speed is reached around 200 MB file size @fig-correlation-filesize-speed-200MB. This is the configured chunk size of s3cmd which is the tool used by nsc-exporter for data transfer. We might want to increase the chunk size to improve the transfer speed?
* The current transfer speed is not optimal considering the 10Gbps switch connecting NSC and TSD. We need to investigate the reason for the low transfer speed.


### Conclusion

* We might be able to run `4 x 48` or `2 x 48 + 1 x 96` samples per week with the current transfer speed. However, we will reach maximum capicity of data transfer.
* If we can increase the transfer speed, e.g. reaching 200MB/s, we can easily double current production capacity.

## Data storage

WGS produces large amount of data. The data storage capacity is critical for the upscaling. 


### NSC
On NSC side, the data is stored in on **boston** at `/boston/diag`. Boston has a total capacity of 1.5 PB, and the usable capacity is 1.2 at the moment.


### TSD

On TSD side, the data is stored in `/cluster/projects/p22`. The total capacity is 1.8 PB, and the usable capacity is 1.2 PB at the moment.



## Pipeline capacity (Illumina DRAGEN)

Illunima DRAGEN is a bioinformatics pipeline server that can be used to process WGS data. It takes around 1 hours to process a 30x WGS sample.


# Discussion
To be addded...


# Conclusion

To be added... 


[^1]: The nsc-exporter log and sequencer overview html files are very small files and do not belong to any projects. They are always transferred in a very short time. They will not affect the transfer speed of other files. Therefore, they are ignored for simplicity.
[^2]: Data comes in a continuous manner. The nsc-exporter normally takes a snapshot of new data and tranfer it. It then sleeps for 10 minutes before checking for new data. The slept times counted here are where next sleep is more than the sleeping interval (10 minutes) later, siginifying that new data comes right after the sleep. This is contrary to long idle time where no new data comes after the sleep.
[^3]: To make sure files are transferred intact, `s3cmd put` checks the *md5sum* of the files. This takes time and is not reported as transfer time by `s3cmd`
