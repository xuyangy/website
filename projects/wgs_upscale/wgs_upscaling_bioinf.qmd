---
title: "WGS Upscaling - IT & Bioinformatics Evaluation"
title-block-banner: true
description: "Data transfer, Data storage, Bioinformatics pipeline capacity"
author: 
    - name: Xuyang Yuan
      email: "xuyangy@uio.no"
      affiliation: "GDx, Oslo University Hospital"
      url: "https://dpipe.gitlab.io/docs/docs-pages"
      role: "collect data"
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
#     fig-cap-location: top
date: last-modified
format: 
    html:
        theme: Journal
        toc: true
        toc-depth: 4
        number-sections: true
        number-depth: 4
        float: true
        header-includes: |
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
execute: 
    cache: true
---

# Background
GDx at OUSAMG is planning to upscale the WGS production to `4 x 48` samples or `2 x 48 + 1 x 96` samples per week. 

This document evaluates the possible bottlenecks of IT & bioinformatics pipelines in following areas:

1. Data transfer speed
2. Data storage
3. Pipeline capacity (Illumina DRAGEN)

# IT && Bioinformatics



## Data transfer speed

```{r}
#| echo: false
#| message: false
#| warning: false
recognize_bandwidth <- function(bandwidth_string) {
  # Use regular expression to extract numeric part and unit part
  match_result <- regexec("^([0-9.]+)\\s*([KMGT]?B)(/s)?$", bandwidth_string)

  if (match_result[[1]][1] == -1) {
    stop("Invalid bandwidth string format")
  }

  # Extract numeric and unit parts
  numeric_part <- as.numeric(regmatches(bandwidth_string, match_result)[[1]][2])
  unit_part <- regmatches(bandwidth_string, match_result)[[1]][3]

  # Convert to bytes per second
  bytes_per_second <- switch(unit_part,
    "B" = numeric_part,
    "KB" = numeric_part * 1000,
    "MB" = numeric_part * 1000^2,
    "GB" = numeric_part * 1000^3,
    stop("Invalid unit")                                     
  )                                                          
                                                             
  return(bytes_per_second)                                   
}                                                            
                                                             
transform_bandwidth <- function(bandwidth) {
  return(lapply(bandwidth, recognize_bandwidth))
}

# load data from csv file
library(readr)               
library(dplyr)               
data <- read_csv("data.csv", col_names = TRUE, col_types = "cccddc")
data <- mutate(data, speed = sapply(speed, recognize_bandwidth))
```



### Data Collection

To evaluate the data transfer speed, we collected the transfer time for each file transferred from NSC to TSD between [`r strptime(min(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"} and [`r strptime(max(data$datetime), "%Y-%m-%d %H:%M:%S")`]{style="font-family:Quicksand;"}.

::: {.callout-note collapse="true"}


## examples of collected data


```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(stringr)
for (extension in c(".bam", ".fastq.gz", ".vcf", ".sample", ".pdf")) {
  print(t(data |> filter(str_ends(data$filename, extension)) |> slice_sample(n = 1)))
}
```

:::


### Overview
::: {.panel-tabset}
#### file size
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(gdata)
setNames(as.data.frame(sapply(summary(data$bytes), humanReadable)), "filesize")
```
:::
::: {.column width="70%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
ggplot(
    data,
    aes(x = bytes)
) +
    geom_histogram(fill = "#677487", bins = 200) +
    labs(x = "bytes", y = "count", title = "filesize")
```
:::
::::
#### transfer speed
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
setNames(as.data.frame(sapply(summary(data$speed), humanReadable)), "speed(/s)")
```
:::
::: {.column width="70%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
ggplot(
    data,
    aes(x = speed)
) + 
    geom_histogram(bins=200, fill="#677487") + 
    labs(x = "bytes/s", y = "count", title = "transfer speed")
```
:::
::::
#### time used (seconds)
:::: {.columns}
::: {.column width="30%"}
```{r} 
#| echo: false
summary(data %>% select(seconds))
```
:::
::: {.column width="70%"}
```{r} 
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
ggplot(
    data,
    aes(x = seconds)
) + 
    geom_histogram(bins=200, fill="#677487") + 
    labs(x = "seconds", y = "count", title = "transfer time")
```
:::
::::
:::

### Plots


<!-- read data from csv file -->
```{python}
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| include: false
def deHumanReadable(bandwidth_string):
    match_result = re.match(r'^([0-9.]+)\s*([KMGT]?B)(/s)?$', bandwidth_string)

    if match_result is None:
        raise ValueError("Invalid bandwidth string format")

    numeric_part = float(match_result.group(1))
    unit_part = match_result.group(2)
    bytes_per_second = {
        "B": numeric_part,
        "KB": numeric_part * 1000,
        "MB": numeric_part * 1000**2,
        "GB": numeric_part * 1000**3
    }.get(unit_part, ValueError("Invalid unit"))

    return bytes_per_second

```

#### Transfer speed and time VS file size (all files)


```{python} 
#| echo: false
#| cache: false
import pandas as pd
from datetime import datetime
import re
import plotly.express as px
py_df = pd.read_csv("data.csv", 
                    sep=",", 
                    parse_dates=["datetime"],
                    date_format="%Y-%m-%d %H:%M:%S.%f")
py_df["speed"] = py_df["speed"].apply(deHumanReadable)
py_df["project_type"] = py_df["project"].apply(lambda x: re.match(r"[a-zA-Z]+", x).group())
fig = px.scatter(py_df, x="bytes", y="speed", opacity=0.3, color="project_type")
fig.show()

fig_time = px.scatter(py_df, x="bytes", y="seconds", opacity=0.3, color="project_type")
fig_time.show()
```

#### Transfer speed and time VS file size (small files)

```{python} 
#| echo: false
import plotly.express as px
small_files = py_df[py_df["bytes"] < 300000]
fig_sf = px.scatter(small_files, x="bytes", y="speed", opacity=0.3, color="project_type")
fig_sf.show()

fig_sf_time = px.scatter(small_files, x="bytes", y="seconds", opacity=0.3, color="project_type")
fig_sf_time.show()
```


#### [ Maximum transfer reached around <u>200MB</u> file size? ]{style="color:orange;"}

```{python} 
#| echo: false
import plotly.express as px
small_files = py_df[py_df["bytes"] < 500000000]
fig_200MB = px.scatter(small_files, x="bytes", y="speed", opacity=0.3, color="project_type")
fig_200MB.show()
```

## Data storage

WGS produces large amount of data. The data storage capacity is critical for the upscaling. 


### NSC
On NSC side, the data is stored in on **boston** at `/boston/diag`. Boston has a total capacity of 1.5 PB, and the usable capacity is 1.2 at the moment.


### TSD

On TSD side, the data is stored in `/cluster/projects/p22`. The total capacity is 1.8 PB, and the usable capacity is 1.2 PB at the moment.



## Pipeline capacity (Illumina DRAGEN)

Illunima DRAGEN is a bioinformatics pipeline server that can be used to process WGS data. It takes around 1 hours to process a 30x WGS sample.




